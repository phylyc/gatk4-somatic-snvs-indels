version development

## Copyright Broad Institute, 2017
## Philipp HÃ¤hnel, 2022
##
## This WDL workflow runs GATK4 Mutect 2 in multi-sample mode on a list of of tumor
## samples with an optional list of paired normal samples, and performs additional
## filtering and functional annotation tasks.
##
## Main requirements/expectations :
## - One analysis-ready BAM file (and its index) for each sample
##
## Description of inputs:
##
## ** Primary inputs **
## ref_fasta, ref_fasta_index, ref_dict: reference genome, index, and dictionary
## tumor_bams, tumor_bais: lists of BAM and index for the tumor samples
## normal_bams, normal_bais: lists of BAM and index for the normal samples
##
## ** Workflow Options **
## run_contaminanation_model:
## run_orientation_bias_mixture_model_filter:
## run_variant_filter:
## run_realignment_filter:
## run_funcotator:
## keep_germline: retains germline variants; currently not supported. They are being
##      filtered by the SelectVariants task.
## compress_output:
## make_bamout:
##
## ** Primary resources ** (optional but strongly recommended)
## panel_of_normals, panel_of_normals_idx: optional panel of normals (and its index)
##      in VCF format containing probable technical artifacts (false positves)
## gnomad, gnomad_vcf_idx: optional database of known germline variants (and its index)
##      (see http://gnomad.broadinstitute.org/downloads)
## variants_for_contamination, variants_for_contamination_idx: VCF of common variants
##      (and its index) with allele frequencies for calculating contamination.
##      (necessary if run_contaminanation_model == true)
##
## ** Secondary resources ** (for optional tasks)
## bwa_mem_index_image: resource for FilterAlignmentArtifacts. Generated by
##      BwaMemIndexImageCreator. (necessary if run_realignment_filter == true)
## funcotator_transcript_list:
## data_sources_tar_gz: resource for Funcotator. If not specified, the most recent one
##      is downloaded and used. (specifying a resource is significantly faster)
##
## ** Runtime **
## gatk_docker: docker image to use for GATK 4 Mutect2
## preemptible: how many preemptions to tolerate before switching to a non-preemptible
##      machine (on Google)
## max_retries: how many times to retry failed tasks -- very important on the cloud when
##      there are transient errors
## gatk_override: (optional) local file or Google bucket path to a GATK 4 java jar file
##      to be used instead of the GATK 4 jar in the docker image. This must be supplied
##      when running in an environment that does not support docker
##      (e.g. SGE cluster on a Broad on-prem VM)
## There are variables for memory assignment of the individual tasks. The default values
## have been optimized so that the workflow runs for all tested cases of multi-sample
## calling, with up to 25 samples.
##
## ** Workflow options **
## interval_list: genomic intervals (optional; will be used for scatter)
##      It is recommended to specify an interval list that has at least the unplaced and
##      unlocalized contigs removed. The realignment filter task may have trouble
##      dealing with them.
## scatter_count: number of parallel jobs to generate when scattering over intervals
##      Low scatter counts have long runtimes for each shard and increase the chance
##      for a preemptible to fail prematurely, thus increasing cost. On the other hand,
##      each shard has an overhead of ~5 minutes for spinup and spindown, which is the
##      main source of compute cost for large scatter counts. The main difference
##      between WES and WGS is in the filtering alignment artifacts task:
##                                  scatter: 10     scatter: 50
##        VariantCall                   2-5h            30m
##        FilterAlignmentArtifacts
##              WES                     25m             15m
##              WGS                     7h
##      For multisample calling, the runtimes significantly increase. With 50 shards on
##      25 samples the VariantCall task runs at least 4 hours, some shards up to 18 hours.
##      With only 10 shards, this runs for more than 48 hours, which amounts to > $100.
##      Tumor-only cases generate ~50k variants, which, if split over only 10 shards,
##      require the alignment artifact filter task to use more memory than the default.
##      TL;DR: When in doubt, use a larger scatter_count.
##
## Outputs :
## - One VCF file and its index with primary filtering applied;
##   secondary filtering and functional annotation if requested;
##   a bamout.bam file of reassembled reads if requested
##
## Cromwell version support
## - Successfully tested on v71
##
## LICENSING :
## This script is released under the WDL source code license (BSD-3) (see LICENSE in
## https://github.com/broadinstitute/wdl). Note however that the programs it calls may
## be subject to different licenses. Users are responsible for checking that they are
## authorized to run all programs before running this script. Please see the docker
## pages at https://hub.docker.com/r/broadinstitute/* for detailed licensing information
## pertaining to the included programs.

struct Runtime {
    String gatk_docker
    File? gatk_override
    Int max_retries
    Int preemptible
    Int cpu
    Int machine_mem
    Int command_mem
    Int disk
    Int boot_disk_size
}

workflow MultiSampleMutect2 {
    input {
        File? interval_list
        File ref_fasta
        File ref_fasta_index
        File ref_dict

        String individual_id
        Array[File]+ tumor_bams
        Array[File]+ tumor_bais
        Array[File]? normal_bams
        Array[File]? normal_bais

        # workflow options
        Boolean run_contamination_model = true
        Boolean run_orientation_bias_mixture_model = true
        Boolean run_variant_filter = true
        Boolean run_realignment_filter = true
        Boolean run_realignment_filter_only_on_high_confidence_variants = false
        Boolean run_funcotator = true
        Boolean keep_germline = false  # not currently supported
        Boolean compress_output = true
        Boolean make_bamout = false
        Boolean funcotator_use_gnomad = true

        # expose extra arguments for import of this workflow
        String? split_intervals_extra_args
        String? m2_extra_args
        String? m2_filter_extra_args
        String? select_variants_extra_args
        String? select_low_conficence_variants_arg = "-select '(vc.getAttribute(\"DP\") < 4) || (vc.getAttribute(\"MBQ\").0 == 0) || (vc.getAttribute(\"MFRL\").0 == 0)'"
        String? realignment_extra_args
        String? funcotate_extra_args

        # resources
        File? panel_of_normals
        File? panel_of_normals_idx
        File? germline_resource
        File? germline_resource_tbi
        File? variants_for_contamination
        File? variants_for_contamination_idx
        File? bwa_mem_index_image
        File? funcotator_transcript_list
        File? funcotator_data_sources_tar_gz

        # runtime
        Int scatter_count = 42
        String gatk_docker = "broadinstitute/gatk"
        File? gatk_override
        Int preemptible = 2
        Int max_retries = 2
        Int emergency_extra_diskGB = 0

        # memory assignments in MB
        Int additional_per_sample_mem = 256  # this actually can depend on bam size (WES vs WGS)
        Int split_intervals_mem = 512  # 64
        Int get_sample_name_mem = 512  # 256
        Int variant_call_base_mem = 4096
        Int learn_read_orientation_model_base_mem = 6144
        Int get_pileup_summaries_mem = 2048  # needs at least 2G
        Int gather_pileup_summaries_mem = 512  # 64
        Int calculate_contamination_mem = 512
        Int filter_mutect_calls_mem = 4096
        Int select_variants_mem = 2048
        Int filter_alignment_artifacts_mem = 4096
        Int merge_vcfs_mem = 512
        Int merge_mutect_stats_mem = 512 # 64
        Int merge_bams_mem = 8192  # wants at least 6G
        Int funcotate_mem = 4096

        # Increasing cpus likely increases costs by the same factor.
        Int variant_call_cpu = 1  # good for PairHMM: 2
        Int filter_alignment_artifacts_cpu = 1  # good for PairHMM: 4
    }

    Array[File] non_optional_normal_bams = select_first([normal_bams, []])
    Array[File] non_optional_normal_bais = select_first([normal_bais, []])

    Boolean normal_is_present = defined(normal_bams) && (length(non_optional_normal_bams) > 0)
    Int num_bams = length(tumor_bams) + length(non_optional_normal_bams)

    # Disk sizes used for dynamic sizing
    Int tumor_size = ceil(size(tumor_bams, "GB") + size(tumor_bais, "GB"))
    Int normal_size = (
        if normal_is_present
        then ceil(size(non_optional_normal_bams, "GB") + size(non_optional_normal_bais, "GB"))
        else 0
    )
    Int ref_size = ceil(size(ref_fasta, "GB") + size(ref_dict, "GB") + size(ref_fasta_index, "GB"))
    Int gatk_override_size = if defined(gatk_override) then ceil(size(gatk_override, "GB")) else 0
    Int disk_padGB = 1 + gatk_override_size + emergency_extra_diskGB
    Int m2_output_size = if make_bamout then ceil(tumor_size / scatter_count) else 0
    Int m2_per_scatter_size = 1 + m2_output_size + disk_padGB

    Runtime standard_runtime = {
        "gatk_docker": gatk_docker,
        "gatk_override": gatk_override,
        "max_retries": max_retries,
        "preemptible": preemptible,
        "cpu": 1,
        "machine_mem": 2024,
        "command_mem": 2024,
        "disk": 1 + disk_padGB,
        "boot_disk_size": 12  # needs to be > 10
    }

    call SplitIntervals {
    	input:
            interval_list = interval_list,
            ref_fasta = ref_fasta,
            ref_fasta_index = ref_fasta_index,
            ref_dict = ref_dict,
            scatter_count = scatter_count,
            split_intervals_extra_args = split_intervals_extra_args,
            runtime_params = standard_runtime,
            memoryMB = split_intervals_mem
    }

    if (normal_is_present) {
        scatter (normal_bam in non_optional_normal_bams) {
            call GetSampleName as GetNormalSampleName {
                input:
                    bam = normal_bam,
                    runtime_params = standard_runtime,
                    memoryMB = get_sample_name_mem
            }
        }
    }

    scatter (scattered_intervals in SplitIntervals.interval_files) {
    	call VariantCall {
            input:
                interval_list = scattered_intervals,
                ref_fasta = ref_fasta,
                ref_fasta_index = ref_fasta_index,
                ref_dict = ref_dict,
                individual_id = individual_id,
                tumor_bams = tumor_bams,
                tumor_bais = tumor_bais,
                normal_bams = normal_bams,
                normal_bais = normal_bais,
                normal_sample_names = GetNormalSampleName.sample_name,
                panel_of_normals = panel_of_normals,
                panel_of_normals_idx = panel_of_normals_idx,
                germline_resource = germline_resource,
                germline_resource_tbi = germline_resource_tbi,
                make_bamout = make_bamout,
                run_ob_filter = run_orientation_bias_mixture_model,
                genotype_germline_sites = keep_germline,
                compress_output = compress_output,
                m2_extra_args = m2_extra_args,
                gatk_docker = gatk_docker,
                gatk_override = gatk_override,
                memoryMB = variant_call_base_mem + num_bams * additional_per_sample_mem,
                cpu = variant_call_cpu,
                disk_spaceGB = m2_per_scatter_size,
		}
	}

    call MergeVCFs as MergeVariantCallVCFs {
    	input:
            input_vcfs = VariantCall.vcf,
            input_vcf_indices = VariantCall.vcf_idx,
            output_name = individual_id + ".unfiltered.merged",
            compress_output = compress_output,
            runtime_params = standard_runtime,
            memoryMB = merge_vcfs_mem
    }

    call MergeMutectStats {
        input:
            stats = VariantCall.vcf_stats,
            individual_id = individual_id,
            runtime_params = standard_runtime,
            memoryMB = merge_mutect_stats_mem
    }

    if (run_contamination_model) {
        scatter (tumor_sample in zip(tumor_bams, tumor_bais)) {
            scatter (scattered_intervals in SplitIntervals.interval_files) {
                call GetPileupSummaries as GetTumorPileupSummaries {
                    input:
                        input_bam = tumor_sample.left,
                        input_bai = tumor_sample.right,
                        interval_list = scattered_intervals,
                        variants_for_contamination = variants_for_contamination,
                        variants_for_contamination_idx = variants_for_contamination_idx,
                        runtime_params = standard_runtime,
                        memoryMB = get_pileup_summaries_mem
                }
            }

            call GatherPileupSummaries as GatherTumorPileupSummaries {
                input:
                    input_tables = flatten(GetTumorPileupSummaries.pileup_summaries),
                    ref_dict = ref_dict,
                    output_name = basename(tumor_sample.left, ".bam") + ".pileup",
                    runtime_params = standard_runtime,
                    memoryMB = gather_pileup_summaries_mem
            }
        }

        if (normal_is_present) {
            # If multiple normals are present, it is not that important which of them to
            # choose for the contamination workflow.
            # todo: Choose the normal with the greatest sequencing depth.
#            call SelectBestNormal {
#                input:
#                    bams = non_optional_normal_bams,
#                    bais = non_optional_normal_bais,
#                    runtime_params = standard_runtime
#            }
            File best_normal_bam = select_first(non_optional_normal_bams)
            File best_normal_bai = select_first(non_optional_normal_bais)

            scatter (scattered_intervals in SplitIntervals.interval_files) {
                call GetPileupSummaries as GetNormalPileupSummaries {
                    input:
                        input_bam = best_normal_bam,
                        input_bai = best_normal_bai,
                        interval_list = scattered_intervals,
                        variants_for_contamination = variants_for_contamination,
                        variants_for_contamination_idx = variants_for_contamination_idx,
                        runtime_params = standard_runtime,
                        memoryMB = get_pileup_summaries_mem
                }
            }

            call GatherPileupSummaries as GatherNormalPileupSummaries {
                input:
                    input_tables = flatten(GetNormalPileupSummaries.pileup_summaries),
                    ref_dict = ref_dict,
                    output_name = basename(best_normal_bam, ".bam") + ".pileup",
                    runtime_params = standard_runtime,
                    memoryMB = gather_pileup_summaries_mem
            }
        }

        scatter (tumor_pileups in GatherTumorPileupSummaries.merged_pileup_summaries) {
            call CalculateContamination {
                input:
                    tumor_pileups = tumor_pileups,
                    normal_pileups = GatherNormalPileupSummaries.merged_pileup_summaries,
                    runtime_params = standard_runtime,
                    memoryMB = calculate_contamination_mem
            }
        }
    }

    if (run_orientation_bias_mixture_model) {
        call LearnReadOrientationModel {
            input:
                individual_id = individual_id,
                f1r2_counts = select_all(VariantCall.m2_artifact_priors),
                runtime_params = standard_runtime,
                memoryMB = learn_read_orientation_model_base_mem + num_bams * additional_per_sample_mem
        }
    }

    if (run_variant_filter) {
        # From the documentation: "FilterMutectCalls goes over an unfiltered vcf in
        # three passes, two to learn any unknown parameters of the filters' models and
        # to set the threshold P(error), and one to apply the learned filters. [...]
        # [As such] it is critical to merge the unfiltered output of Mutect2 before
        # filtering."
        call FilterMutectCalls {
            input:
                ref_fasta = ref_fasta,
                ref_fasta_index = ref_fasta_index,
                ref_dict = ref_dict,
                input_vcf = MergeVariantCallVCFs.merged_vcf,
                input_vcf_idx = MergeVariantCallVCFs.merged_vcf_idx,
                orientation_bias = LearnReadOrientationModel.orientation_bias,
                contamination_tables = CalculateContamination.contamination_table,
                tumor_segmentation = CalculateContamination.tumor_segmentation,
                mutect_stats = MergeMutectStats.merged_stats,
                compress_output = compress_output,
                m2_filter_extra_args = m2_filter_extra_args,
                runtime_params = standard_runtime,
                memoryMB = filter_mutect_calls_mem,
        }

        call SelectVariants as SelectPassingVariants {
            input:
                filtered_vcf = FilterMutectCalls.filtered_vcf,
                filtered_vcf_idx = FilterMutectCalls.filtered_vcf_idx,
                exclude_filtered = true,
                keep_germline = keep_germline,
                compress_output = compress_output,
                select_variants_extra_args = select_variants_extra_args,
                runtime_params = standard_runtime,
                memoryMB = select_variants_mem
        }

        if (run_realignment_filter) {
            # Realigning reads for variants can be expensive if many variants have been
            # called. Especially for tumor-only calling, plenty of variant calls are
            # still sequencing artifacts of sometimes obviously low quality that have
            # been missed by FilterMutectCalls. In order to make the filter affordable,
            # we divide the called variants into low and high confidence groups based
            # on read depth and VAF. Variants that come from reads that only support the
            # alternate allele are suspect. For those variants, the MBQ and MFRL are set
            # to zero.
            if (run_realignment_filter_only_on_high_confidence_variants) {
                call SelectVariants as SelectLowConfidenceVariants {
                    input:
                        filtered_vcf = SelectPassingVariants.selected_vcf,
                        filtered_vcf_idx = SelectPassingVariants.selected_vcf_idx,
                        compress_output = compress_output,
                        select_variants_extra_args = select_low_conficence_variants_arg,
                        runtime_params = standard_runtime,
                        memoryMB = select_variants_mem
                }

                call SelectVariants as SelectHighConfidenceVariants {
                    input:
                        filtered_vcf = SelectPassingVariants.selected_vcf,
                        filtered_vcf_idx = SelectPassingVariants.selected_vcf_idx,
                        compress_output = compress_output,
                        select_variants_extra_args = select_low_conficence_variants_arg + " -invertSelect true",
                        runtime_params = standard_runtime,
                        memoryMB = select_variants_mem
                }
            }

            File variants_to_realign = select_first([
                if run_realignment_filter_only_on_high_confidence_variants
                then SelectHighConfidenceVariants.selected_vcf
                else SelectPassingVariants.selected_vcf
            ])
            File variants_to_realign_idx = select_first([
                if run_realignment_filter_only_on_high_confidence_variants
                then SelectHighConfidenceVariants.selected_vcf_idx
                else SelectPassingVariants.selected_vcf_idx
            ])

            # Due to its long runtime, we scatter the realignment task over intervals.
            scatter (scattered_intervals in SplitIntervals.interval_files) {
                call SelectVariants as SelectPreRealignmentVariants {
                    input:
                        interval_list = scattered_intervals,
                        ref_fasta = ref_fasta,
                        ref_fasta_index = ref_fasta_index,
                        ref_dict = ref_dict,
                        filtered_vcf = variants_to_realign,
                        filtered_vcf_idx = variants_to_realign_idx,
                        compress_output = compress_output,
                        runtime_params = standard_runtime,
                        memoryMB = select_variants_mem
                }

                call FilterAlignmentArtifacts {
                    input:
                        ref_fasta = ref_fasta,
                        ref_fasta_index = ref_fasta_index,
                        ref_dict = ref_dict,
                        tumor_bams = tumor_bams,
                        tumor_bais = tumor_bais,
                        input_vcf = SelectPreRealignmentVariants.selected_vcf,
                        input_vcf_idx = SelectPreRealignmentVariants.selected_vcf_idx,
                        bwa_mem_index_image = bwa_mem_index_image,
                        compress_output = compress_output,
                        realignment_extra_args = realignment_extra_args,
                        runtime_params = standard_runtime,
                        memoryMB = filter_alignment_artifacts_mem,
                        cpu = filter_alignment_artifacts_cpu
                }
            }

            call MergeVCFs as MergeRealignmentFilteredVCFs {
                input:
                    input_vcfs = FilterAlignmentArtifacts.filtered_vcf,
                    input_vcf_indices = FilterAlignmentArtifacts.filtered_vcf_idx,
                    output_name = individual_id + ".filtered.selected.realignmentfiltered",
                    compress_output = compress_output,
                    runtime_params = standard_runtime,
                    memoryMB = merge_vcfs_mem
            }

            call SelectVariants as SelectPostRealignmentVariants {
                input:
                    filtered_vcf = MergeRealignmentFilteredVCFs.merged_vcf,
                    filtered_vcf_idx = MergeRealignmentFilteredVCFs.merged_vcf_idx,
                    exclude_filtered = true,
                    keep_germline = keep_germline,
                    compress_output = compress_output,
                    select_variants_extra_args = select_variants_extra_args,
                    runtime_params = standard_runtime,
                    memoryMB = select_variants_mem
            }

            if (run_realignment_filter_only_on_high_confidence_variants) {
                call MergeVCFs as MergeLowConfidenceAndRealignmentFilteredVCFs {
                    input:
                        input_vcfs = select_all([
                            SelectLowConfidenceVariants.selected_vcf,
                            SelectPostRealignmentVariants.selected_vcf
                        ]),
                        input_vcf_indices = select_all([
                            SelectLowConfidenceVariants.selected_vcf_idx,
                            SelectPostRealignmentVariants.selected_vcf_idx
                        ]),
                        output_name = individual_id + ".filtered.selected.realignmentfiltered.selected.merged",
                        compress_output = compress_output,
                        runtime_params = standard_runtime,
                        memoryMB = merge_vcfs_mem
                }
            }

            File realignment_filtered_variants = select_first([
                if run_realignment_filter_only_on_high_confidence_variants
                then MergeLowConfidenceAndRealignmentFilteredVCFs.merged_vcf
                else SelectPostRealignmentVariants.selected_vcf
            ])
            File realignment_filtered_variants_idx = select_first([
                if run_realignment_filter_only_on_high_confidence_variants
                then MergeLowConfidenceAndRealignmentFilteredVCFs.merged_vcf_idx
                else SelectPostRealignmentVariants.selected_vcf_idx
            ])
        }
    }

    File selected_vcf = select_first([
        if run_variant_filter then (
            if run_realignment_filter then realignment_filtered_variants
            else SelectPassingVariants.selected_vcf
        ) else MergeVariantCallVCFs.merged_vcf
    ])
    File selected_vcf_idx = select_first([
        if run_variant_filter then (
            if run_realignment_filter then realignment_filtered_variants_idx
            else SelectPassingVariants.selected_vcf_idx
        ) else MergeVariantCallVCFs.merged_vcf_idx
    ])

    String vcf_name = basename(basename(selected_vcf, ".gz"), ".vcf")

    if (make_bamout) {
        call MergeBamOuts {
            input:
                ref_fasta = ref_fasta,
                ref_fasta_index = ref_fasta_index,
                ref_dict = ref_dict,
                m2_bam_outs = select_all(VariantCall.bamout),
                output_vcf_name = vcf_name,
                runtime_params = standard_runtime,
                memoryMB = merge_bams_mem
        }
    }

    if (run_funcotator) {
        # Funcotator is not designed to differentiate between samples. The tool uses the
        # position and allele information to get annotations but does not annotate per
        # sample. We use a workaround by splitting the multi-sample VCF into individual
        # samples and run Funcotator on each VCF.
        # In order to get proper annotations for the normal samples, they should be
        # called in tumor-only mode.
        scatter (tumor_bam_pair in zip(tumor_bams, tumor_bais)) {
            File tumor_bam = tumor_bam_pair.left
            File tumor_bai = tumor_bam_pair.right

            call GetSampleName as GetTumorSampleName {
                input:
                    bam = tumor_bam,
                    runtime_params = standard_runtime,
                    memoryMB = get_sample_name_mem
            }
            # We select the first normal sample to be the matched normal.
            # todo: select normal with greatest sequencing depth
            String normal_sample_name = (
                if normal_is_present
                then select_first(select_first([GetNormalSampleName.sample_name]))
                else "Unknown"
            )

            call SelectVariants as SelectSampleVariants {
                input:
                    filtered_vcf = selected_vcf,
                    filtered_vcf_idx = selected_vcf_idx,
                    exclude_filtered = false,  # is already filtered
                    keep_germline = keep_germline,
                    tumor_sample_name = GetTumorSampleName.sample_name,
                    normal_sample_name = if normal_sample_name != "Unknown" then normal_sample_name else None,
                    compress_output = compress_output,
                    select_variants_extra_args = select_variants_extra_args,
                    runtime_params = standard_runtime,
                    memoryMB = select_variants_mem
            }

            call Funcotate {
                input:
                    ref_fasta = ref_fasta,
                    ref_fasta_index = ref_fasta_index,
                    ref_dict = ref_dict,
                    interval_list = interval_list,
                    input_vcf = SelectSampleVariants.selected_vcf,
                    input_vcf_idx = SelectSampleVariants.selected_vcf_idx,
                    output_base_name = GetTumorSampleName.sample_name + ".annotated",
                    tumor_sample_name = GetTumorSampleName.sample_name,
                    normal_sample_name = if normal_sample_name != "Unknown" then normal_sample_name else None,
                    transcript_list = funcotator_transcript_list,
                    data_sources_tar_gz = funcotator_data_sources_tar_gz,
                    use_gnomad = funcotator_use_gnomad,
                    compress_output = compress_output,
                    funcotate_extra_args = funcotate_extra_args,
                    runtime_params = standard_runtime,
                    memoryMB = funcotate_mem
            }
        }
    }

    output {
        File unfiltered_vcf = MergeVariantCallVCFs.merged_vcf
        File unfiltered_vcf_idx = MergeVariantCallVCFs.merged_vcf_idx
        File merged_vcf = selected_vcf
        File merged_vcf_idx = selected_vcf_idx
        File mutect_stats = MergeMutectStats.merged_stats
        File? bamout = MergeBamOuts.merged_bam_out
        File? bamout_index = MergeBamOuts.merged_bam_out_index
        File? filtering_stats = FilterMutectCalls.filtering_stats
        File? read_orientation_model_params = LearnReadOrientationModel.orientation_bias
        Array[File]? contamination_table = CalculateContamination.contamination_table
        Array[File]? tumor_segmentation = CalculateContamination.tumor_segmentation
        Array[File?]? funcotated_file = Funcotate.funcotated_output_file
        Array[File?]? funcotated_file_index = Funcotate.funcotated_output_file_index
    }
}

task SplitIntervals {
    input {
        File? interval_list
        File ref_fasta
        File ref_fasta_index
        File ref_dict
        Int scatter_count
        String? split_intervals_extra_args

        Runtime runtime_params
        Int? memoryMB = 64
    }

    String extra_args = (
        select_first([split_intervals_extra_args, ""])
        # to avoid splitting intervals:
        # + " --subdivision-mode BALANCING_WITHOUT_INTERVAL_SUBDIVISION_WITH_OVERFLOW"
        # Applied after inital scatter, so leads to more scattered intervals.
        # + " --dont-mix-contigs"
    )

    parameter_meta {
        interval_list: {localization_optional: true}
        ref_fasta: {localization_optional: true}
        ref_fasta_index: {localization_optional: true}
        ref_dict: {localization_optional: true}
    }

    command <<<
        set -e
        export GATK_LOCAL_JAR=~{default="/root/gatk.jar" runtime_params.gatk_override}
        mkdir interval-files
        gatk --java-options "-Xmx~{select_first([memoryMB, runtime_params.command_mem])}m" \
            SplitIntervals \
            -R ~{ref_fasta} \
            ~{"-L " + interval_list} \
            -scatter ~{scatter_count} \
            -O interval-files \
            ~{extra_args}
        cp interval-files/*.interval_list .
    >>>

    output {
        Array[File] interval_files = glob("*.interval_list")
    }

    runtime {
        docker: runtime_params.gatk_docker
        bootDiskSizeGb: runtime_params.boot_disk_size
        memory: select_first([memoryMB, runtime_params.machine_mem]) + " MB"
        disks: "local-disk " + runtime_params.disk + " HDD"
        preemptible: runtime_params.preemptible
        maxRetries: runtime_params.max_retries
        cpu: runtime_params.cpu
    }
}

task GetSampleName {
    input {
        File bam

        Runtime runtime_params
        Int? memoryMB = 256
    }

    parameter_meta {
        bam: {localization_optional: true}
    }

    command <<<
        set -e
        export GATK_LOCAL_JAR=~{default="/root/gatk.jar" runtime_params.gatk_override}
        gatk --java-options "-Xmx~{select_first([memoryMB, runtime_params.command_mem])}m" \
            GetSampleName \
            -I ~{bam} \
            -O bam_name.txt \
            -encode
        cat bam_name.txt
    >>>

    output {
        String sample_name = read_string(stdout())
    }

    runtime {
        docker: runtime_params.gatk_docker
        bootDiskSizeGb: runtime_params.boot_disk_size
        memory: select_first([memoryMB, runtime_params.machine_mem]) + " MB"
        disks: "local-disk " + runtime_params.disk + " HDD"
        preemptible: runtime_params.preemptible
        maxRetries: runtime_params.max_retries
        cpu: runtime_params.cpu
    }
}

task VariantCall {
    input {
        File? interval_list
        File ref_fasta
        File ref_fasta_index
        File ref_dict

        String individual_id
        Array[File] tumor_bams
        Array[File] tumor_bais
        Array[File]? normal_bams
        Array[File]? normal_bais
        Array[String]? normal_sample_names

        File? panel_of_normals
        File? panel_of_normals_idx
        File? germline_resource
        File? germline_resource_tbi

        Boolean genotype_germline_sites = false
        Boolean native_pair_hmm_use_double_precision = false
        Boolean use_linked_de_bruijn_graph = true

        String? m2_extra_args

        Boolean run_ob_filter = false
        Boolean compress_output = false
        Boolean make_bamout = false

        File? gatk_override
        String gatk_docker
        Int preemptible = 2
        Int max_retries = 2
        Int cpu = 2  # to have 4 native_hmm_pair_threads
        Int memoryMB = 8192
        Int disk_spaceGB = 100
        Int boot_disk_size_GB = 12  # must be > 10
    }

    parameter_meta {
        interval_list: {localization_optional: true}
        ref_fasta: {localization_optional: true}
        ref_fasta_index: {localization_optional: true}
        ref_dict: {localization_optional: true}
        tumor_bams: {localization_optional: true}
        tumor_bais: {localization_optional: true}
        normal_bams: {localization_optional: true}
        normal_bais: {localization_optional: true}
        panel_of_normals: {localization_optional: true}
        panel_of_normals_idx: {localization_optional: true}
        germline_resource: {localization_optional: true}
        germline_resource_tbi: {localization_optional: true}
    }

    Boolean normal_is_present = defined(normal_bams) && (length(select_first([normal_bams])) > 0)

    String output_vcf = individual_id + if compress_output then ".vcf.gz" else ".vcf"
    String output_vcf_idx = output_vcf + if compress_output then ".tbi" else ".idx"
    String output_stats = output_vcf + ".stats"

    Int native_hmm_pair_threads = 2 * cpu
    Int germline_resource_size = if defined(germline_resource) then ceil(size(germline_resource, "GB")) else 0
    Int disk_space = disk_spaceGB + germline_resource_size

    String output_bam = individual_id + "_bamout.bam"
    String output_bai = individual_id + "_bamout.bai"
    String make_bamout_arg = if make_bamout then "--bam-output " + output_bam else ""
    String output_artifact_priors = individual_id + "_f1r2_counts.tar.gz"
    String run_ob_filter_arg = if run_ob_filter then "--f1r2-tar-gz " + output_artifact_priors else ""

    command <<<
        set -e
        export GATK_LOCAL_JAR=~{default="/root/gatk.jar" gatk_override}
        gatk --java-options "-Xmx~{memoryMB}m" \
            Mutect2 \
            --reference ~{ref_fasta} \
            ~{sep=" " prefix("-I ", tumor_bams)} \
            ~{true="-I " false="" normal_is_present}~{default="" sep=" -I " normal_bams} \
            ~{true="-normal " false="" normal_is_present}~{default="" sep=" -normal " normal_sample_names} \
            --output ~{output_vcf} \
            ~{"--intervals " + interval_list} \
            ~{"-pon " + panel_of_normals} \
            ~{make_bamout_arg} \
            ~{run_ob_filter_arg} \
            ~{"--germline-resource " + germline_resource} \
            ~{true="--genotype-germline-sites true" false="" genotype_germline_sites} \
            ~{true="--linked-de-bruijn-graph true" false="" use_linked_de_bruijn_graph} \
            --smith-waterman FASTEST_AVAILABLE \
            --pair-hmm-implementation FASTEST_AVAILABLE \
            ~{"--native-pair-hmm-threads " + native_hmm_pair_threads} \
            ~{true="--native-pair-hmm-use-double-precision true" false="" native_pair_hmm_use_double_precision} \
            --seconds-between-progress-updates 300 \
            ~{m2_extra_args}
    >>>

    output {
        File vcf = output_vcf
        File vcf_idx = output_vcf_idx
        File vcf_stats = output_stats
        File? bamout = output_bam
        File? baiout = output_bai
        File? m2_artifact_priors = output_artifact_priors
    }

    runtime {
        docker: gatk_docker
        bootDiskSizeGb: boot_disk_size_GB
        memory: memoryMB + " MB"
        disks: "local-disk " + disk_space + " HDD"
        preemptible: preemptible
        maxRetries: max_retries
        cpu: native_hmm_pair_threads
    }
}

task MergeMutectStats {
    input {
        Array[File]+ stats
        String individual_id

        Runtime runtime_params
        Int? memoryMB = 256
    }

    # Optional localization leads to cromwell error.
    # parameter_meta {
    #     stats: {localization_optional: true}
    # }

    String output_name = individual_id + "_merged.stats"

    command <<<
        set -e
        export GATK_LOCAL_JAR=~{default="/root/gatk.jar" runtime_params.gatk_override}
        gatk --java-options "-Xmx~{select_first([memoryMB, runtime_params.command_mem])}m" \
            MergeMutectStats \
            ~{sep=" " prefix("-stats ", stats)} \
            --output ~{output_name}
    >>>

    output {
        File merged_stats = output_name
    }

    runtime {
        docker: runtime_params.gatk_docker
        bootDiskSizeGb: runtime_params.boot_disk_size
        memory: select_first([memoryMB, runtime_params.machine_mem]) + " MB"
        disks: "local-disk " + runtime_params.disk + " HDD"
        preemptible: runtime_params.preemptible
        maxRetries: runtime_params.max_retries
        cpu: runtime_params.cpu
    }
}

task LearnReadOrientationModel {
    # This tool uses the forward and reverse read counts collected in the variant call
    # workflow. The basic idea of this tool is that true variants will be supported by
    # reads on both strands of DNA; whereas, artifacts will be supported by reads heavily
    # biased to one of the two strands. CollectF1R2 counts the number of reads on each
    # strand of DNA at a putatively variant site and stores this information along with
    # the nucleotide context. LearnReadOrientationModel uses the information from CollectF1R2
    # to estimate a prior probability that a site with a given context suffers from an
    # artifact. The two main artifacts of concern are OXOG (a result of sequencing technology)
    # and FFPE.

    input {
        String individual_id
        Array[File] f1r2_counts

        Runtime runtime_params
        Int? memoryMB = 8192
    }

    # Optional localization leads to cromwell error.
    # parameter_meta {
    #     f1r2_counts: {localization_optional: true}
    # }

    String output_name = individual_id + "_artifact_priors.tar.gz"
    Boolean f1r2_counts_empty = (length(f1r2_counts) == 0)

    command <<<
        set -e
        export GATK_LOCAL_JAR=~{default="/root/gatk.jar" runtime_params.gatk_override}

        if ~{f1r2_counts_empty} ; then
            echo "ERROR: f1r2_counts_tar_gz must be supplied and non empty."
            false
        fi

        gatk --java-options "-Xmx~{select_first([memoryMB, runtime_params.command_mem])}m" \
            LearnReadOrientationModel \
            ~{sep=" " prefix("-I ", f1r2_counts)} \
            --output ~{output_name}
    >>>

    output {
        File orientation_bias = output_name
    }

    runtime {
        docker: runtime_params.gatk_docker
        bootDiskSizeGb: runtime_params.boot_disk_size
        memory: select_first([memoryMB, runtime_params.machine_mem]) + " MB"
        disks: "local-disk " + runtime_params.disk + " HDD"
        preemptible: runtime_params.preemptible
        maxRetries: runtime_params.max_retries
        cpu: runtime_params.cpu
    }
}

task GetPileupSummaries {
    # If the variants for contamination and the intervals for this scatter don't
    # intersect, GetPileupSummaries throws an error. However, there is nothing wrong
    # with an empty intersection for our purposes; it simply doesn't contribute to the
    # merged pileup summaries that we create downstream. We implement this with an array
    # outputs. If the tool errors, no table is created and the glob yields an empty array.

	input {
        File interval_list
        File input_bam
        File input_bai
        File? variants_for_contamination
        File? variants_for_contamination_idx
        String? getpileupsummaries_extra_args

        Runtime runtime_params
        Int? memoryMB = 2048
        Int? disk_spaceGB
	}

    parameter_meta {
        interval_list: {localization_optional: true}
        input_bam: {localization_optional: true}
        input_bai: {localization_optional: true}
        variants_for_contamination: {localization_optional: true}
        variants_for_contamination_idx: {localization_optional: true}
    }

    String sample_id = basename(input_bam, ".bam")
    String output_name = sample_id + ".pileup"

    command <<<
        set +e
        export GATK_LOCAL_JAR=~{default="/root/gatk.jar" runtime_params.gatk_override}

        if ~{!defined(variants_for_contamination)} ; then
            echo "ERROR: variants_for_contamination must be supplied."
            false
        fi

        gatk --java-options "-Xmx~{select_first([memoryMB, runtime_params.command_mem])}m" \
            GetPileupSummaries \
            --input ~{input_bam} \
            --intervals ~{interval_list} \
            --intervals ~{variants_for_contamination} \
            --interval-set-rule INTERSECTION \
            --variant ~{variants_for_contamination} \
            --output ~{output_name} \
            ~{getpileupsummaries_extra_args}

        # It only fails due to empty intersection between variants and intervals, which is ok.
        exit 0
    >>>

    output {
        Array[File] pileup_summaries = glob(output_name)
    }

    runtime {
        docker: runtime_params.gatk_docker
        bootDiskSizeGb: runtime_params.boot_disk_size
        memory: select_first([memoryMB, runtime_params.machine_mem]) + " MB"
        disks: "local-disk " + select_first([disk_spaceGB, runtime_params.disk]) + " HDD"
        preemptible: runtime_params.preemptible
        maxRetries: runtime_params.max_retries
        cpu: runtime_params.cpu
    }
}

task GatherPileupSummaries {
    input {
        Array[File] input_tables
        File ref_dict
        String output_name

        Runtime runtime_params
        Int? memoryMB = 64
    }

    # Optional localization leads to cromwell error.
    # parameter_meta {
    #     input_tables: {localization_optional: true}
    #     ref_dict: {localization_optional: true}
    # }

    command <<<
        set -e
        export GATK_LOCAL_JAR=~{default="/root/gatk.jar" runtime_params.gatk_override}
        gatk --java-options "-Xmx~{select_first([memoryMB, runtime_params.command_mem])}m" \
            GatherPileupSummaries \
            --sequence-dictionary ~{ref_dict} \
            ~{sep=" " prefix("-I ", input_tables)} \
            -O ~{output_name}
    >>>

    output {
        File merged_pileup_summaries = output_name
    }

    runtime {
        docker: runtime_params.gatk_docker
        bootDiskSizeGb: runtime_params.boot_disk_size
        memory: select_first([memoryMB, runtime_params.machine_mem]) + " MB"
        disks: "local-disk " + runtime_params.disk + " HDD"
        preemptible: runtime_params.preemptible
        maxRetries: runtime_params.max_retries
        cpu: runtime_params.cpu
    }
}

task CalculateContamination {
    # This tool is set, by default, to operate over the intersection of your germling
    # biallelic resource with your specified intervals. The tool works by pulling down
    # alt and ref allele counts at locuses defined in the biallelic_germline_resource
    # and comparing the number of observed alt counts compared to the number of expected
    # alt counts at sites with low population allele frequencies. Using this comparison
    # of observed vs expected, the tool estimates contamination. It gains further
    # accuracy if a matched normal is specified.

    input {
        File tumor_pileups
        File? normal_pileups

        Runtime runtime_params
        Int? memoryMB = 512
    }

    # Optional localization leads to cromwell error.
    # parameter_meta {
    #     tumor_pileups: {localization_optional: true}
    #     normal_pileups: {localization_optional: true}
    # }

    String tumor_sample_id = basename(tumor_pileups, ".pileup")
    String normal_sample_id = if defined(normal_pileups) then "." + basename(select_first([normal_pileups]), ".pileup") else ""
    String output_contamination = tumor_sample_id + normal_sample_id + ".contamination"
    String output_segments = tumor_sample_id + normal_sample_id + ".segments"

    command <<<
        set -e
        export GATK_LOCAL_JAR=~{default="/root/gatk.jar" runtime_params.gatk_override}
        gatk --java-options "-Xmx~{select_first([memoryMB, runtime_params.command_mem])}m" \
            CalculateContamination \
            --input ~{tumor_pileups} \
            ~{"--matched-normal " + normal_pileups} \
            --output ~{output_contamination} \
            --tumor-segmentation ~{output_segments}
    >>>

    output {
        File contamination_table = output_contamination
        File tumor_segmentation = output_segments
    }

    runtime {
        docker: runtime_params.gatk_docker
        bootDiskSizeGb: runtime_params.boot_disk_size
        memory: select_first([memoryMB, runtime_params.machine_mem]) + " MB"
        disks: "local-disk " + runtime_params.disk + " HDD"
        preemptible: runtime_params.preemptible
        maxRetries: runtime_params.max_retries
        cpu: runtime_params.cpu
    }
}

task FilterMutectCalls {
    # Passing variants are labelled with PASS annotation.

    input {
        File ref_fasta
        File ref_fasta_index
        File ref_dict

        File input_vcf
        File input_vcf_idx
        File? orientation_bias
        Array[File]? contamination_tables
        Array[File]? tumor_segmentation
        File? mutect_stats

        Int max_median_fragment_length_difference = 10000  # default: 10000
        Int min_alt_median_base_quality = 20  # default: 20
        Int min_alt_median_mapping_quality = 20  # default: -1

        Boolean compress_output = false
        String? m2_filter_extra_args

        Runtime runtime_params
        Int? memoryMB = 4096
    }

    # Optional localization leads to cromwell error.
    parameter_meta{
        ref_fasta: {localization_optional: true}
        ref_fasta_index: {localization_optional: true}
        ref_dict: {localization_optional: true}
        input_vcf: {localization_optional: true}
        input_vcf_idx: {localization_optional: true}
        # orientation_bias: {localization_optional: true}
        # contamination_tables: {localization_optional: true}
        # tumor_segmentation: {localization_optional: true}
        # mutect_stats: {localization_optional: true}
    }

    Int disk_spaceGB = (
        ceil(size(input_vcf, "GB"))
        + if defined(orientation_bias) then ceil(size(orientation_bias, "GB")) else 0
        + if defined(contamination_tables) then ceil(size(contamination_tables, "GB")) else 0
        + if defined(tumor_segmentation) then ceil(size(tumor_segmentation, "GB")) else 0
        + if defined(mutect_stats) then ceil(size(mutect_stats, "GB")) else 0
        + runtime_params.disk
    )

    String output_base_name = basename(basename(input_vcf, ".gz"), ".vcf") + ".filtered"
    String output_vcf = output_base_name + if compress_output then ".vcf.gz" else ".vcf"
    String output_vcf_idx = output_vcf + if compress_output then ".tbi" else ".idx"

    command <<<
        set -e
        export GATK_LOCAL_JAR=~{default="/root/gatk.jar" runtime_params.gatk_override}
        gatk --java-options "-Xmx~{select_first([memoryMB, runtime_params.command_mem])}m" \
            FilterMutectCalls \
            --reference ~{ref_fasta} \
            --variant ~{input_vcf} \
            --output ~{output_vcf} \
            ~{"--orientation-bias-artifact-priors " + orientation_bias} \
            ~{true="--contamination-table " false="" defined(contamination_tables)}~{default="" sep=" --contamination-table " contamination_tables} \
            ~{true="--tumor-segmentation " false="" defined(tumor_segmentation)}~{default="" sep=" --tumor-segmentation " tumor_segmentation} \
            ~{"--stats " + mutect_stats} \
            ~{"--max-median-fragment-length-difference " + max_median_fragment_length_difference} \
            ~{"--min-median-base-quality " + min_alt_median_base_quality} \
            ~{"--min-median-mapping-quality " + min_alt_median_mapping_quality} \
            --filtering-stats ~{output_base_name}.stats \
            --seconds-between-progress-updates 300 \
            ~{m2_filter_extra_args}
    >>>

    output {
        File filtered_vcf = output_vcf
        File filtered_vcf_idx = output_vcf_idx
        File filtering_stats = output_base_name + ".stats"
    }

    runtime {
        docker: runtime_params.gatk_docker
        bootDiskSizeGb: runtime_params.boot_disk_size
        memory: select_first([memoryMB, runtime_params.machine_mem]) + " MB"
        disks: "local-disk " + disk_spaceGB + " HDD"
        preemptible: runtime_params.preemptible
        maxRetries: runtime_params.max_retries
        cpu: runtime_params.cpu
    }
}

task FilterAlignmentArtifacts {
    # Lifts the reads supporting the variant alleles from the tumor bams and aligns
    # these reads to Hg38. If the reads supporting the variant allele have ambiguous
    # alignments, the tool will mark these variants as filtered in the VCF.

    input {
        File ref_fasta
        File ref_fasta_index
        File ref_dict
        File input_vcf
        File input_vcf_idx
        Array[File]+ tumor_bams
        Array[File]+ tumor_bais

        File? bwa_mem_index_image

        Int max_reasonable_fragment_length = 10000 # default: 100000
        Boolean compress_output = false
        String? realignment_extra_args

        Runtime runtime_params
        String? gatk_docker
        Int? memoryMB = 4096
        Int? cpu = 2  # wants 4
    }

    parameter_meta {
        input_vcf: {localization_optional: true}
        input_vcf_idx: {localization_optional: true}
        tumor_bams: {localization_optional: true}
        tumor_bais: {localization_optional: true}
        ref_fasta: {localization_optional: true}
        ref_fasta_index: {localization_optional: true}
        ref_dict: {localization_optional: true}
        # bwa_mem_index_image: {localization_optional: true}  # needs to be localized
    }

    Int disk_spaceGB = ceil(size(bwa_mem_index_image, "GB")) + runtime_params.disk

    String output_base_name = basename(basename(input_vcf, ".gz"), ".vcf") + ".realignmentfiltered"
    String output_vcf = output_base_name + if compress_output then ".vcf.gz" else ".vcf"
    String output_vcf_idx = output_vcf + if compress_output then ".tbi" else ".idx"

    command <<<
        set -e
        export GATK_LOCAL_JAR=~{default="/root/gatk.jar" runtime_params.gatk_override}

        if ~{!defined(bwa_mem_index_image)} ; then
            echo "ERROR: bwa_mem_index_image must be supplied."
            false
        fi

        # Not skipping filtered variants is important for keeping germline variants if
        # requested.
        gatk --java-options "-Xmx~{select_first([memoryMB, runtime_params.command_mem])}m" \
            FilterAlignmentArtifacts \
            ~{sep=" " prefix("-I ", tumor_bams)} \
            --variant ~{input_vcf} \
            --reference ~{ref_fasta} \
            --bwa-mem-index-image ~{bwa_mem_index_image} \
            --output ~{output_vcf} \
            --max-reasonable-fragment-length ~{max_reasonable_fragment_length} \
            --dont-skip-filtered-variants true \
            ~{realignment_extra_args}
    >>>

    output {
        File filtered_vcf = output_vcf
        File filtered_vcf_idx = output_vcf_idx
    }

    runtime {
        docker: select_first([gatk_docker, runtime_params.gatk_docker])
        bootDiskSizeGb: runtime_params.boot_disk_size
        memory: select_first([memoryMB, runtime_params.machine_mem]) + " MB"
        disks: "local-disk " + disk_spaceGB + " HDD"
        preemptible: runtime_params.preemptible
        maxRetries: runtime_params.max_retries
        cpu: cpu
    }
}

task SelectVariants {
    # Selects only variants with the PASS annotation given in Filter Mutect Calls.
    # todo: add functionality to keep germline variants

    input {
        File? interval_list
        File? ref_fasta
        File? ref_fasta_index
        File? ref_dict
        File filtered_vcf
        File filtered_vcf_idx
        Boolean exclude_filtered = false
        Boolean keep_germline = false
        Boolean compress_output = false
        String? tumor_sample_name
        String? normal_sample_name
        String? select_variants_extra_args

        Runtime runtime_params
        Int? memoryMB = 512
    }

    parameter_meta {
        ref_fasta: {localization_optional: true}
        ref_fasta_index: {localization_optional: true}
        ref_dict: {localization_optional: true}
        filtered_vcf: {localization_optional: true}
        filtered_vcf_idx: {localization_optional: true}
    }

    String uncompressed_filtered_vcf = basename(filtered_vcf, ".gz")
    String base_name = if defined(tumor_sample_name) then tumor_sample_name else basename(uncompressed_filtered_vcf, ".vcf")
    String output_base_name = base_name + ".selected"
    String uncompressed_output_vcf = output_base_name + ".vcf"
    String output_vcf = uncompressed_output_vcf + if compress_output then ".gz" else ""
    String output_vcf_idx = output_vcf + if compress_output then ".tbi" else ".idx"

    String dollar = "$"
    # Remove variants from multi-sample calling that are not present in the selected
    # tumor sample. We demand that the total read depth is greater than the allelic
    # depth of the reference allele. This ensures that there is at least one alternate
    # allele present.
    String select_true_variants_arg = (
        if defined(tumor_sample_name)
        then "-select 'vc.getGenotype(\"" + tumor_sample_name + "\").getAD().0 < vc.getGenotype(\"" + tumor_sample_name + "\").getDP()'"
        else ""
    )

    command <<<
        set -e
        export GATK_LOCAL_JAR=~{default="/root/gatk.jar" runtime_params.gatk_override}
        gatk --java-options "-Xmx~{select_first([memoryMB, runtime_params.command_mem])}m" \
            SelectVariants \
            ~{"-R " + ref_fasta} \
            ~{"-L " + interval_list} \
            -V ~{filtered_vcf} \
            --output ~{uncompressed_output_vcf} \
            --exclude-filtered ~{exclude_filtered} \
            ~{"--sample-name " + tumor_sample_name} \
            ~{"--sample-name " + normal_sample_name} \
            ~{select_true_variants_arg} \
            ~{select_variants_extra_args}

        # =======================================
        # Hack to correct a SelectVariants output bug. When selecting for samples, this
        # task only retains the first sample annotation in the header. Those annotations
        # are important for Funcotator to fill the t_alt_count and t_ref_count coverage
        # columns.
        # Note: This can possibly be achieved by adding an appropriate --COMMENT flag.

        if ~{defined(tumor_sample_name)} ; then
            echo ">> Fixing tumor sample name in vcf header ... "
            input_header=~{dollar}(grep "##tumor_sample=" ~{uncompressed_output_vcf})
            corrected_header="##tumor_sample=~{tumor_sample_name}"
            sed -i "s/~{dollar}input_header/~{dollar}corrected_header/g" ~{uncompressed_output_vcf}
        fi
        if ~{defined(normal_sample_name)} ; then
            echo ">> Fixing normal sample name in vcf header ... "
            input_header=~{dollar}(grep "##normal_sample=" ~{uncompressed_output_vcf})
            corrected_header="##normal_sample=~{normal_sample_name}"
            sed -i "s/~{dollar}input_header/~{dollar}corrected_header/g" ~{uncompressed_output_vcf}
        fi
        if ~{compress_output} ; then
            echo ">> Compressing selected vcf."
            bgzip -c ~{uncompressed_output_vcf} > ~{output_vcf}
            gatk --java-options "-Xmx~{select_first([memoryMB, runtime_params.command_mem])}m" \
                IndexFeatureFile \
                --input ~{output_vcf} \
                --output ~{output_vcf_idx}
        fi
    >>>

    output {
        File selected_vcf = output_vcf
        File selected_vcf_idx = output_vcf_idx
    }

    runtime {
        docker: runtime_params.gatk_docker
        bootDiskSizeGb: runtime_params.boot_disk_size
        memory: select_first([memoryMB, runtime_params.machine_mem]) + " MB"
        disks: "local-disk " + runtime_params.disk + " HDD"
        preemptible: runtime_params.preemptible
        maxRetries: runtime_params.max_retries
        cpu: runtime_params.cpu
    }
}

task MergeVCFs {
    # Consider replacing MergeVcfs with GatherVcfsCloud once the latter is out of beta.

	input {
        Array[File] input_vcfs
        Array[File] input_vcf_indices
        String output_name
        Boolean compress_output = false

        Runtime runtime_params
        Int? memoryMB = 512
    }

    # Optional localization leads to cromwell error.
    # parameter_meta {
    #     input_vcfs: {localization_optional: true}
    #     input_vcf_indices: {localization_optional: true}
    # }

    String output_vcf = output_name + ".vcf" + if compress_output then ".gz" else ""
    String output_vcf_idx = output_vcf + if compress_output then ".tbi" else ".idx"

    command <<<
        set -e
        export GATK_LOCAL_JAR=~{default="/root/gatk.jar" runtime_params.gatk_override}
        gatk --java-options "-Xmx~{select_first([memoryMB, runtime_params.command_mem])}m" \
            MergeVcfs \
            ~{sep=" " prefix("-I ", input_vcfs)} \
            -O ~{output_vcf}
    >>>

    output {
    	File merged_vcf = output_vcf
        File merged_vcf_idx = output_vcf_idx
    }

    runtime {
        docker: runtime_params.gatk_docker
        bootDiskSizeGb: runtime_params.boot_disk_size
        memory: select_first([memoryMB, runtime_params.machine_mem]) + " MB"
        disks: "local-disk " + runtime_params.disk + " HDD"
        preemptible: runtime_params.preemptible
        maxRetries: runtime_params.max_retries
        cpu: runtime_params.cpu
    }
}

task MergeBamOuts {
    input {
        File ref_fasta
        File ref_fasta_index
        File ref_dict
        Array[File]+ m2_bam_outs
        String output_vcf_name

        Runtime runtime_params
        Int? memoryMB = 6144
        Boolean use_ssd = false
    }

    parameter_meta {
        ref_fasta: {localization_optional: true}
        ref_fasta_index: {localization_optional: true}
        ref_dict: {localization_optional: true}
        # m2_bam_outs: {localization_optional: true}  # samtools requires localization
    }

    Int disk_spaceGB = 4 * ceil(size(m2_bam_outs, "GB")) + runtime_params.disk

    command <<<
        set -e
        export GATK_LOCAL_JAR=~{default="/root/gatk.jar" runtime_params.gatk_override}
        gatk --java-options "-Xmx~{select_first([memoryMB, runtime_params.command_mem])}m" \
            GatherBamFiles \
            ~{sep=" " prefix("-I ", m2_bam_outs)} \
            -O unsorted.out.bam \
            -R ~{ref_fasta}

        # We must sort because adjacent scatters may have overlapping (padded) assembly
        # regions, hence overlapping bamouts

        gatk --java-options "-Xmx~{select_first([memoryMB, runtime_params.command_mem])}m" \
            SortSam \
            -I unsorted.out.bam \
            -O ~{output_vcf_name}.out.bam \
            --SORT_ORDER coordinate \
            -VALIDATION_STRINGENCY LENIENT

        gatk --java-options "-Xmx~{select_first([memoryMB, runtime_params.command_mem])}m" \
            BuildBamIndex \
            -I ~{output_vcf_name}.out.bam \
            -VALIDATION_STRINGENCY LENIENT
    >>>

    output {
        File merged_bam_out = "~{output_vcf_name}.out.bam"
        File merged_bam_out_index = "~{output_vcf_name}.out.bai"
    }

    runtime {
        docker: runtime_params.gatk_docker
        bootDiskSizeGb: runtime_params.boot_disk_size
        memory: select_first([memoryMB, runtime_params.machine_mem]) + " MB"
        disks: "local-disk " + disk_spaceGB + if use_ssd then " SSD" else " HDD"
        preemptible: runtime_params.preemptible
        maxRetries: runtime_params.max_retries
        cpu: runtime_params.cpu
    }
}

task Funcotate {
    input {
        File ref_fasta
        File ref_fasta_index
        File ref_dict
        File? interval_list

        File input_vcf
        File input_vcf_idx
        String? tumor_sample_name
        String? normal_sample_name

        String reference_version = "hg19"
        String output_base_name
        String output_format = "MAF"
        String variant_type = "somatic"  # alternative: germline
        String transcript_selection_mode = "CANONICAL"  # GATK default: "CANONICAL"
        File? transcript_list
        File? data_sources_tar_gz  # most recent version is downloaded if not chosen
        Boolean use_gnomad = false
        Boolean compress_output = false
        Array[String]? annotation_defaults
        Array[String]? annotation_overrides
        Array[String]? exclude_fields
        String? funcotate_extra_args

        Runtime runtime_params
        Int? memoryMB = 4096
        Int? disk_spaceGB
        Boolean use_ssd = false
    }

    parameter_meta {
        ref_fasta: {localization_optional: true}
        ref_fasta_index: {localization_optional: true}
        ref_dict: {localization_optional: true}
        interval_list: {localization_optional: true}
        input_vcf: {localization_optional: true}
        input_vcf_idx: {localization_optional: true}
        # transcript_selection_file: {localization_optional: true}
    }

    # ==============
    # Process input args:
    String output_maf = output_base_name + ".maf"
    String output_maf_index = output_maf + ".idx"
    String output_vcf = output_base_name + if compress_output then ".vcf.gz" else ".vcf"
    String output_vcf_idx = output_vcf +  if compress_output then ".tbi" else ".idx"
    String output_file = if output_format == "MAF" then output_maf else output_vcf
    String output_file_index = if output_format == "MAF" then output_maf_index else output_vcf_idx

    # Calculate disk size:
    Int funco_tar_sizeGB = if defined(data_sources_tar_gz) then 3 * ceil(size(data_sources_tar_gz, "GB")) else 100
    Int diskGB = funco_tar_sizeGB + select_first([disk_spaceGB, runtime_params.disk])

    String dollar = "$"

    command <<<
        set -e
        export GATK_LOCAL_JAR=~{default="/root/gatk.jar" runtime_params.gatk_override}

        # =======================================
        # Hack to validate the WDL inputs:
        #
        # NOTE: This happens here so that we don't waste time copying down the data
        # sources if there's an error.

        if [[ "~{output_format}" != "MAF" ]] && [[ "~{output_format}" != "VCF" ]] ; then
            echo "ERROR: Output format must be MAF or VCF."
            false
        fi

        mkdir datasources_dir
        DATA_SOURCES_FOLDER="$PWD/datasources_dir"
        echo "Obtaining Funcotator data sources..."
        if [[ ! -z "~{data_sources_tar_gz}" ]]; then
            data_sources_tar_gz=~{data_sources_tar_gz}
        else
            data_sources_tar_gz="funcotator_datasources.tar.gz"
            gatk --java-options "-Xmx~{select_first([memoryMB, runtime_params.command_mem])}m" \
                FuncotatorDataSourceDownloader \
                --~{variant_type} \
                --validate-integrity \
                --output ~{dollar}data_sources_tar_gz
        fi
        echo "Unzipping Funcotator data sources..."
        tar zxvf ~{dollar}data_sources_tar_gz -C datasources_dir --strip-components 1

        if ~{use_gnomad} ; then
            echo "Enabling gnomAD..."
            for potential_gnomad_gz in gnomAD_exome.tar.gz gnomAD_genome.tar.gz ; do
                if [[ -f ~{dollar}{DATA_SOURCES_FOLDER}/~{dollar}{potential_gnomad_gz} ]] ; then
                    cd ~{dollar}{DATA_SOURCES_FOLDER}
                    tar -zvxf ~{dollar}{potential_gnomad_gz}
                    cd -
                else
                    echo "ERROR: Cannot find gnomAD folder: ~{dollar}{potential_gnomad_gz}" 1>&2
                    false
                fi
            done
        fi

        gatk --java-options "-Xmx~{select_first([memoryMB, runtime_params.command_mem])}m" \
            Funcotator \
            --data-sources-path $DATA_SOURCES_FOLDER \
            --ref-version ~{reference_version} \
            --output-file-format ~{output_format} \
            -R ~{ref_fasta} \
            -V ~{input_vcf} \
            -O ~{output_file} \
            ~{"-L " + interval_list} \
            ~{"--transcript-selection-mode " + transcript_selection_mode} \
            ~{"--transcript-list " + transcript_list} \
            --annotation-default tumor_barcode:~{default="Unknown" tumor_sample_name} \
            --annotation-default normal_barcode:~{default="Unknown" normal_sample_name} \
            ~{true="--annotation-default " false="" defined(annotation_defaults)}~{default="" sep=" --annotation-default " annotation_defaults} \
            ~{true="--annotation-override " false="" defined(annotation_overrides)}~{default="" sep=" --annotation-override " annotation_overrides} \
            ~{true="--exclude-field " false="" defined(exclude_fields)}~{default="" sep=" --exclude-field " exclude_fields} \
            ~{funcotate_extra_args}

        # Make sure we have a placeholder index for MAF files so this workflow doesn't fail:
        if [[ "~{output_format}" == "MAF" ]] ; then
            touch ~{output_maf_index}
        fi
    >>>

    output {
        File funcotated_output_file = output_file
        File funcotated_output_file_index = output_file_index
    }

    runtime {
        docker: runtime_params.gatk_docker
        bootDiskSizeGb: runtime_params.boot_disk_size
        memory: select_first([memoryMB, runtime_params.machine_mem]) + " MB"
        disks: "local-disk " + diskGB + if use_ssd then " SSD" else " HDD"
        preemptible: runtime_params.preemptible
        maxRetries: runtime_params.max_retries
        cpu: runtime_params.cpu
    }
}