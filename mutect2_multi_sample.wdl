version development

## Copyright Broad Institute, 2017
## Philipp HÃ¤hnel, 2022
##
## This WDL workflow runs GATK4 Mutect 2 in multi-sample mode on a list of of tumor
## samples with an optional list of paired normal samples, and performs additional
## filtering and functional annotation tasks.
##
## Main requirements/expectations :
## - One analysis-ready BAM file (and its index) for each sample
##
## Description of inputs:
##
## ** Primary inputs **
## ref_fasta, ref_fasta_index, ref_dict: reference genome, index, and dictionary
## tumor_bams, tumor_bais: lists of BAM and index for the tumor samples
## normal_bams, normal_bais: lists of BAM and index for the normal samples
##
## ** Workflow Options **
## run_contaminanation_model:
## run_orientation_bias_mixture_model_filter:
## run_variant_filter:
## run_realignment_filter:
## run_realignment_filter_only_on_high_confidence_variants:
## select_low_conficence_variants_jexl_arg: JEXL filtering expression to select low
##      confidence somatic variants. See the default value for formatting details.
## run_cnn_scoring_model: Setting it to true will probably fail. Need to wait for
##      updates to the tool.
## run_funcotator:
##
## compress_output:
## make_bamout:
## funcotator_use_gnomad:
## genotype_germline_sites: Use with care! https://github.com/broadinstitute/gatk/issues/7391
## genotype_pon_sites:
## native_pair_hmm_use_double_precision:
## use_linked_de_bruijn_graph:
## recover_all_dangling_branches:
##
## ** Primary resources ** (optional but strongly recommended)
## panel_of_normals, panel_of_normals_idx: optional panel of normals (and its index)
##      in VCF format containing probable technical artifacts (false positves)
## gnomad, gnomad_vcf_idx: optional database of known germline variants (and its index)
##      (see http://gnomad.broadinstitute.org/downloads)
## variants_for_contamination, variants_for_contamination_idx: VCF of common variants
##      (and its index) with allele frequencies for calculating contamination.
##      (necessary if run_contaminanation_model == true)
##
## ** Secondary resources ** (for optional tasks)
## bwa_mem_index_image: resource for FilterAlignmentArtifacts. Generated by
##      BwaMemIndexImageCreator. (necessary if run_realignment_filter == true)
## funcotator_transcript_list:
## data_sources_tar_gz: resource for Funcotator. If not specified, the most recent one
##      is downloaded and used. (specifying a resource is significantly faster)
##
## ** Runtime **
## gatk_docker: docker image to use for GATK 4 Mutect2
## preemptible: how many preemptions to tolerate before switching to a non-preemptible
##      machine (on Google)
## max_retries: how many times to retry failed tasks -- very important on the cloud when
##      there are transient errors
## gatk_override: (optional) local file or Google bucket path to a GATK 4 java jar file
##      to be used instead of the GATK 4 jar in the docker image. This must be supplied
##      when running in an environment that does not support docker
##      (e.g. SGE cluster on a Broad on-prem VM)
## There are variables for memory assignment of the individual tasks. The default values
## have been optimized so that the workflow runs for all tested cases of multi-sample
## calling, with up to 25 samples.
##
## ** Workflow options **
## interval_list: genomic intervals (optional; will be used for scatter)
##      It is recommended to specify an interval list that has at least the unplaced and
##      unlocalized contigs removed. The realignment filter task may have trouble
##      dealing with them.
## interval_lists: Array of genomic intervals (optional)
##      All intervals will be merged.
## scatter_count: number of parallel jobs to generate when scattering over intervals
##      Low scatter counts have long runtimes for each shard and increase the chance
##      for a preemptible to fail prematurely, thus increasing cost. On the other hand,
##      each shard has an overhead of ~5 minutes for spinup and spindown, which is the
##      main source of compute cost for large scatter counts. The main difference
##      between WES and WGS is in the filtering alignment artifacts task.
##      For multisample calling, the runtimes significantly increase. With 50 shards on
##      25 samples the VariantCall task runs at least 4 hours, some shards up to 18 hours.
##      Tumor-only cases generate ~50k variants, which, if split over only less than
##      the default number of shards (42), require the alignment artifact filter task to
##      use more memory than the default.
##      TL;DR: When in doubt, use a larger scatter_count.
##
## Outputs :
## - One VCF file and its index with primary filtering applied;
##   secondary filtering and functional annotation if requested;
##   a bamout.bam file of reassembled reads if requested
##
## Cromwell version support
## - Successfully tested on v71
##
## LICENSING :
## This script is released under the WDL source code license (BSD-3) (see LICENSE in
## https://github.com/broadinstitute/wdl). Note however that the programs it calls may
## be subject to different licenses. Users are responsible for checking that they are
## authorized to run all programs before running this script. Please see the docker
## pages at https://hub.docker.com/r/broadinstitute/* for detailed licensing information
## pertaining to the included programs.

import "https://github.com/phylyc/gatk4-somatic-snvs-indels/raw/master/eval_intervals.wdl" as eval_i


struct GATKRuntime {
    String gatk_docker
    File? gatk_override
    Int max_retries
    Int preemptible
    Int cpu
    Int machine_mem
    Int command_mem
    Int runtime_minutes
    Int disk
    Int boot_disk_size
}

workflow MultiSampleMutect2 {
    input {
        File? interval_list
        Array[File]? interval_lists
        File ref_fasta
        File ref_fasta_index
        File ref_dict

        String individual_id
        Array[File]+ tumor_bams
        Array[File]+ tumor_bais
        Array[File]? normal_bams
        Array[File]? normal_bais
        Array[String]? tumor_sample_names
        Array[String]? normal_sample_names

        # resources
        File? panel_of_normals
        File? panel_of_normals_idx
        File? germline_resource
        File? germline_resource_tbi
        File? variants_for_contamination
        File? variants_for_contamination_idx
        File? bwa_mem_index_image
        File? funcotator_transcript_list
        File? funcotator_data_sources_tar_gz

        # workflow options
        Boolean call_covered_regions_only = true
        Boolean run_contamination_model = true
        Boolean run_orientation_bias_mixture_model = true
        Boolean run_variant_filter = true
        Boolean run_realignment_filter = true
        Boolean run_realignment_filter_only_on_high_confidence_variants = false
        Boolean run_cnn_scoring_model = false  # likely leads to failure if true; better performance with make_bamout = true
        Boolean run_funcotator = true

        Boolean compress_output = true
        Boolean make_bamout = false

        Boolean genotype_germline_sites = false  # use with care!
        Boolean genotype_pon_sites = false  # use with care!

        # arguments
        Int preprocess_intervals_bin_length = 0
        Int preprocess_intervals_padding = 0
        Int min_read_depth_threshold = 1
        Int get_evaluation_intervals_bin_length = 0
        Int get_evaluation_intervals_padding = 0
        Boolean mutect2_native_pair_hmm_use_double_precision = true
        Boolean mutect2_use_linked_de_bruijn_graph = true
        Boolean mutect2_recover_all_dangling_branches = true
        Int mutect2_downsampling_stride = 50  # default 1
        Int mutect2_max_reads_per_alignment_start = 100  # default: 50
        Int filter_mutect2_max_median_fragment_length_difference = 10000  # default: 10000
        Int filter_mutect2_min_alt_median_base_quality = 20  # default: 20
        Int filter_mutect2_min_alt_median_mapping_quality = 20  # default: -1
        Int filter_alignment_artifacts_max_reasonable_fragment_length = 10000 # default: 100000
        String funcotator_reference_version = "hg19"
        String funcotator_output_format = "MAF"
        String funcotator_variant_type = "somatic"  # alternative: germline
        String funcotator_transcript_selection_mode = "CANONICAL"  # GATK default: "CANONICAL"
        Boolean funcotator_use_gnomad = true
        Array[String]? funcotator_data_sources_paths
        Array[String]? funcotator_annotation_defaults
        Array[String]? funcotator_annotation_overrides
        Array[String]? funcotator_exclude_fields

        # expose extra arguments for import of this workflow
        String? split_intervals_extra_args
        String? mutect2_extra_args
        String? filter_mutect2_extra_args
        String? select_variants_extra_args
        String? select_low_conficence_variants_jexl_arg = "'(vc.getAttribute(\"GERMQ\") < 30) || (vc.getAttribute(\"DP\") < 4) || (vc.getAttribute(\"MBQ\").0 == 0) || (vc.getAttribute(\"MFRL\").0 == 0)'"
        String? realignment_extra_args
        String? funcotate_extra_args

        # runtime
        Int scatter_count = 10
        String gatk_docker = "broadinstitute/gatk"
        File? gatk_override
        Int preemptible = 1
        Int max_retries = 1
        Int emergency_extra_diskGB = 0

        # memory assignments in MB
        Int mem_additional_per_sample = 256  # this actually can depend on bam size (WES vs WGS)
        Int mem_preprocess_intervals = 2048
        Int mem_get_genome_coverage = 4096
        Int mem_get_evaluation_intervals = 4096
        Int mem_split_intervals = 512
        Int mem_get_sample_name = 512
        Int mem_variant_call_base = 4096
        Int mem_learn_read_orientation_model_base = 6144
        Int mem_get_pileup_summaries = 4096  # needs at least 2G
        Int mem_gather_pileup_summaries = 512  # 64
        Int mem_calculate_contamination = 8192  # depends on the variants_for_contamination resource
        Int mem_filter_mutect_calls = 4096
        Int mem_select_variants = 1024
        Int mem_filter_alignment_artifacts = 2048  # needs to be increased in some cases
        Int mem_merge_vcfs = 512
        Int mem_merge_mutect_stats = 512 # 64
        Int mem_merge_bams = 8192  # wants at least 6G
        Int mem_cnn_scoring = 4096
        Int mem_funcotate = 6144

        # runtime assignments in minutes (for HPC cluster)
        Int time_startup = 10
        Int time_preprocess_intervals = 60
        Int time_get_genome_coverage = 60
        Int time_get_evaluation_intervals = 60
        Int time_split_intervals = 1
        Int time_get_sample_name = 1
        Int time_variant_call_total = 10000  # 6 d / scatter
        Int time_learn_read_orientation_model = 180  # 3 h
        Int time_get_pileup_summaries = 90  # 1.5 h
        Int time_gather_pileup_summaries = 5
        Int time_calculate_contamination = 10
        Int time_filter_mutect_calls = 800  # 13 h
        Int time_select_variants = 5
        Int time_filter_alignment_artifacts_total = 10000  # 12 d / scatter
        Int time_merge_vcfs = 10
        Int time_merge_mutect_stats = 1
        Int time_merge_bams = 60
        Int time_cnn_scoring = 10
        Int time_funcotate = 500  # 8 h

        # Increasing cpus likely increases costs by the same factor.
        Int variant_call_cpu = 1  # good for PairHMM: 2
        Int filter_alignment_artifacts_cpu = 1  # good for PairHMM: 4
        Int cnn_scoring_cpu = 1
    }

    Array[File] non_optional_normal_bams = select_first([normal_bams, []])
    Array[File] non_optional_normal_bais = select_first([normal_bais, []])

    Boolean normal_is_present = defined(normal_bams) && (length(non_optional_normal_bams) > 0)
    Int num_bams = length(tumor_bams) + length(non_optional_normal_bams)

    # Disk sizes used for dynamic sizing
    Int tumor_size = ceil(size(tumor_bams, "GB") + size(tumor_bais, "GB"))
    Int normal_size = (
        if normal_is_present
        then ceil(size(non_optional_normal_bams, "GB") + size(non_optional_normal_bais, "GB"))
        else 0
    )
    Int ref_size = ceil(size(ref_fasta, "GB") + size(ref_dict, "GB") + size(ref_fasta_index, "GB"))
    Int gatk_override_size = if defined(gatk_override) then ceil(size(gatk_override, "GB")) else 0
    Int disk_padGB = 1 + gatk_override_size + emergency_extra_diskGB
    Int m2_output_size = if make_bamout then ceil(tumor_size / scatter_count) else 0
    Int m2_per_scatter_size = 1 + m2_output_size + disk_padGB

    GATKRuntime standard_runtime = {
        "gatk_docker": gatk_docker,
        "gatk_override": gatk_override,
        "max_retries": max_retries,
        "preemptible": preemptible,
        "cpu": 1,
        "machine_mem": 2024,
        "command_mem": 2024,
        "runtime_minutes": 60,
        "disk": 1 + disk_padGB,
        "boot_disk_size": 12  # needs to be > 10
    }

    scatter (tumor_bam in tumor_bams) {
        call GetSampleName as GetTumorSampleName {
            input:
                bam = tumor_bam,
                runtime_params = standard_runtime,
                memoryMB = mem_get_sample_name,
                runtime_minutes = time_startup + time_get_sample_name
        }
    }

    if (normal_is_present) {
        scatter (normal_bam in non_optional_normal_bams) {
            call GetSampleName as GetNormalSampleName {
                input:
                    bam = normal_bam,
                    runtime_params = standard_runtime,
                    memoryMB = mem_get_sample_name,
                    runtime_minutes = time_startup + time_get_sample_name
            }
        }
    }

    Array[String] non_optional_tumor_sample_names = select_first([tumor_sample_names, GetTumorSampleName.sample_name])

    call PreprocessIntervals {
        input:
            interval_list = interval_list,
            interval_lists = interval_lists,
            ref_fasta = ref_fasta,
            ref_fasta_index = ref_fasta_index,
            ref_dict = ref_dict,
            bin_length = preprocess_intervals_bin_length,
            padding = preprocess_intervals_padding,
            runtime_params = standard_runtime,
            memoryMB = mem_preprocess_intervals,
            runtime_minutes = time_startup + time_preprocess_intervals
    }

    if (call_covered_regions_only) {
        call eval_i.EvaluationIntervals {
            input:
                interval_list = PreprocessIntervals.preprocessed_interval_list,
                ref_fasta = ref_fasta,
                ref_fasta_index = ref_fasta_index,
                ref_dict = ref_dict,
                collection_name = individual_id,
                sample_names = non_optional_tumor_sample_names,
                input_bams = tumor_bams,
                input_bais = tumor_bais,
                intervals_bin_length = get_evaluation_intervals_bin_length,
                intervals_padding = get_evaluation_intervals_padding,
                gatk_docker = gatk_docker,
                gatk_override = gatk_override,
                preemptible = preemptible,
                max_retries = max_retries,
                emergency_extra_diskGB = emergency_extra_diskGB,
                mem_get_genome_coverage = mem_get_genome_coverage,
                mem_get_evaluation_intervals = mem_get_evaluation_intervals,
                time_startup = time_startup,
                time_get_genome_coverage = time_get_genome_coverage,
                time_get_evaluation_intervals = time_get_evaluation_intervals
        }
    }

    File evaluation_interval_list = select_first([
        PreprocessIntervals.preprocessed_interval_list,
        EvaluationIntervals.evaluation_intervals
    ])

    call SplitIntervals {
    	input:
            interval_list = evaluation_interval_list,
            ref_fasta = ref_fasta,
            ref_fasta_index = ref_fasta_index,
            ref_dict = ref_dict,
            scatter_count = scatter_count,
            split_intervals_extra_args = split_intervals_extra_args,
            runtime_params = standard_runtime,
            memoryMB = mem_split_intervals,
            runtime_minutes = time_startup + time_split_intervals
    }

    scatter (scattered_intervals in SplitIntervals.interval_files) {
    	call VariantCall {
            input:
                interval_list = scattered_intervals,
                ref_fasta = ref_fasta,
                ref_fasta_index = ref_fasta_index,
                ref_dict = ref_dict,
                individual_id = individual_id,
                tumor_bams = tumor_bams,
                tumor_bais = tumor_bais,
                normal_bams = normal_bams,
                normal_bais = normal_bais,
                normal_sample_names = GetNormalSampleName.sample_name,
                panel_of_normals = panel_of_normals,
                panel_of_normals_idx = panel_of_normals_idx,
                germline_resource = germline_resource,
                germline_resource_tbi = germline_resource_tbi,
                make_bamout = make_bamout,
                run_ob_filter = run_orientation_bias_mixture_model,
                compress_output = compress_output,
                genotype_germline_sites = genotype_germline_sites,
                genotype_pon_sites = genotype_pon_sites,
                native_pair_hmm_use_double_precision = mutect2_native_pair_hmm_use_double_precision,
                use_linked_de_bruijn_graph = mutect2_use_linked_de_bruijn_graph,
                recover_all_dangling_branches = mutect2_recover_all_dangling_branches,
                downsampling_stride = mutect2_downsampling_stride,
                max_reads_per_alignment_start = mutect2_max_reads_per_alignment_start,
                m2_extra_args = mutect2_extra_args,
                gatk_docker = gatk_docker,
                gatk_override = gatk_override,
                memoryMB = mem_variant_call_base + num_bams * mem_additional_per_sample,
                runtime_minutes = time_startup + ceil(time_variant_call_total / scatter_count),
                cpu = variant_call_cpu,
                disk_spaceGB = m2_per_scatter_size
		}
	}

    call MergeVCFs as MergeVariantCallVCFs {
    	input:
            input_vcfs = VariantCall.vcf,
            input_vcf_indices = VariantCall.vcf_idx,
            output_name = individual_id + ".unfiltered.merged",
            compress_output = compress_output,
            runtime_params = standard_runtime,
            memoryMB = mem_merge_vcfs,
            runtime_minutes = time_startup + time_merge_vcfs
    }

    call MergeMutectStats {
        input:
            stats = VariantCall.vcf_stats,
            individual_id = individual_id,
            runtime_params = standard_runtime,
            memoryMB = mem_merge_mutect_stats,
            runtime_minutes = time_startup + time_merge_mutect_stats
    }

    if (run_contamination_model) {
        scatter (tumor_sample in zip(tumor_bams, tumor_bais)) {
            scatter (scattered_intervals in SplitIntervals.interval_files) {
                call GetPileupSummaries as GetTumorPileupSummaries {
                    input:
                        input_bam = tumor_sample.left,
                        input_bai = tumor_sample.right,
                        interval_list = scattered_intervals,
                        variants_for_contamination = variants_for_contamination,
                        variants_for_contamination_idx = variants_for_contamination_idx,
                        runtime_params = standard_runtime,
                        memoryMB = mem_get_pileup_summaries,
                        runtime_minutes = time_startup + time_get_pileup_summaries
                }
            }

            call GatherPileupSummaries as GatherTumorPileupSummaries {
                input:
                    input_tables = flatten(GetTumorPileupSummaries.pileup_summaries),
                    ref_dict = ref_dict,
                    output_name = basename(tumor_sample.left, ".bam") + ".pileup",
                    runtime_params = standard_runtime,
                    memoryMB = mem_gather_pileup_summaries,
                    runtime_minutes = time_startup + time_gather_pileup_summaries
            }
        }

        if (normal_is_present) {
            # If multiple normals are present, it is not that important which of them to
            # choose for the contamination workflow.
            # todo: Choose the normal with the greatest sequencing depth.
#            call SelectBestNormal {
#                input:
#                    bams = non_optional_normal_bams,
#                    bais = non_optional_normal_bais,
#                    runtime_params = standard_runtime
#            }
            File best_normal_bam = select_first(non_optional_normal_bams)
            File best_normal_bai = select_first(non_optional_normal_bais)

            scatter (scattered_intervals in SplitIntervals.interval_files) {
                call GetPileupSummaries as GetNormalPileupSummaries {
                    input:
                        input_bam = best_normal_bam,
                        input_bai = best_normal_bai,
                        interval_list = scattered_intervals,
                        variants_for_contamination = variants_for_contamination,
                        variants_for_contamination_idx = variants_for_contamination_idx,
                        runtime_params = standard_runtime,
                        memoryMB = mem_get_pileup_summaries,
                        runtime_minutes = time_startup + time_get_pileup_summaries
                }
            }

            call GatherPileupSummaries as GatherNormalPileupSummaries {
                input:
                    input_tables = flatten(GetNormalPileupSummaries.pileup_summaries),
                    ref_dict = ref_dict,
                    output_name = basename(best_normal_bam, ".bam") + ".pileup",
                    runtime_params = standard_runtime,
                    memoryMB = mem_gather_pileup_summaries,
                    runtime_minutes = time_startup + time_gather_pileup_summaries
            }
        }

        scatter (tumor_pileups in GatherTumorPileupSummaries.merged_pileup_summaries) {
            call CalculateContamination {
                input:
                    tumor_pileups = tumor_pileups,
                    normal_pileups = GatherNormalPileupSummaries.merged_pileup_summaries,
                    runtime_params = standard_runtime,
                    memoryMB = mem_calculate_contamination,
                    runtime_minutes = time_startup + time_calculate_contamination
            }
        }
    }

    if (run_orientation_bias_mixture_model) {
        call LearnReadOrientationModel {
            input:
                individual_id = individual_id,
                f1r2_counts = select_all(VariantCall.m2_artifact_priors),
                runtime_params = standard_runtime,
                memoryMB = mem_learn_read_orientation_model_base + num_bams * mem_additional_per_sample,
                runtime_minutes = time_startup + time_learn_read_orientation_model
        }
    }

    if (run_variant_filter) {
        # From the documentation: "FilterMutectCalls goes over an unfiltered vcf in
        # three passes, two to learn any unknown parameters of the filters' models and
        # to set the threshold P(error), and one to apply the learned filters. [...]
        # [As such] it is critical to merge the unfiltered output of Mutect2 before
        # filtering."
        call FilterMutectCalls {
            input:
                ref_fasta = ref_fasta,
                ref_fasta_index = ref_fasta_index,
                ref_dict = ref_dict,
                input_vcf = MergeVariantCallVCFs.merged_vcf,
                input_vcf_idx = MergeVariantCallVCFs.merged_vcf_idx,
                orientation_bias = LearnReadOrientationModel.orientation_bias,
                contamination_tables = CalculateContamination.contamination_table,
                tumor_segmentation = CalculateContamination.tumor_segmentation,
                mutect_stats = MergeMutectStats.merged_stats,
                max_median_fragment_length_difference = filter_mutect2_max_median_fragment_length_difference,
                min_alt_median_base_quality = filter_mutect2_min_alt_median_base_quality,
                min_alt_median_mapping_quality = filter_mutect2_min_alt_median_mapping_quality,
                compress_output = compress_output,
                m2_filter_extra_args = filter_mutect2_extra_args,
                runtime_params = standard_runtime,
                memoryMB = mem_filter_mutect_calls,
                runtime_minutes = time_startup + time_filter_mutect_calls
        }

        call SelectVariants as SelectPassingVariants {
            input:
                filtered_vcf = FilterMutectCalls.filtered_vcf,
                filtered_vcf_idx = FilterMutectCalls.filtered_vcf_idx,
                exclude_filtered = true,
                compress_output = compress_output,
                select_variants_extra_args = select_variants_extra_args,
                runtime_params = standard_runtime,
                memoryMB = mem_select_variants,
                runtime_minutes = time_startup + time_select_variants
        }

        if (run_realignment_filter) {
            # Realigning reads for variants can be expensive if many variants have been
            # called. Especially for tumor-only calling, plenty of variant calls are
            # still sequencing artifacts of sometimes obviously low quality that have
            # been missed by FilterMutectCalls. In order to make the filter affordable,
            # we divide the called variants into low and high confidence groups based
            # on read depth and VAF. Variants that come from reads that only support the
            # alternate allele are suspect. For those variants, the MBQ and MFRL are set
            # to zero.
            if (run_realignment_filter_only_on_high_confidence_variants) {
                call SelectVariants as SelectLowConfidenceVariants {
                    input:
                        filtered_vcf = SelectPassingVariants.selected_vcf,
                        filtered_vcf_idx = SelectPassingVariants.selected_vcf_idx,
                        compress_output = compress_output,
                        select_variants_extra_args = "-select " + select_low_conficence_variants_jexl_arg,
                        runtime_params = standard_runtime,
                        memoryMB = mem_select_variants,
                        runtime_minutes = time_startup + time_select_variants
                }

                call SelectVariants as SelectHighConfidenceVariants {
                    input:
                        filtered_vcf = SelectPassingVariants.selected_vcf,
                        filtered_vcf_idx = SelectPassingVariants.selected_vcf_idx,
                        compress_output = compress_output,
                        select_variants_extra_args = "-select " + select_low_conficence_variants_jexl_arg + " -invertSelect true",
                        runtime_params = standard_runtime,
                        memoryMB = mem_select_variants,
                        runtime_minutes = time_startup + time_select_variants
                }
            }

            File variants_to_realign = select_first([
                SelectHighConfidenceVariants.selected_vcf,
                SelectPassingVariants.selected_vcf
            ])
            File variants_to_realign_idx = select_first([
                SelectHighConfidenceVariants.selected_vcf_idx,
                SelectPassingVariants.selected_vcf_idx
            ])

            # Due to its long runtime, we scatter the realignment task over intervals.
            scatter (scattered_intervals in SplitIntervals.interval_files) {
                call SelectVariants as SelectPreRealignmentVariants {
                    input:
                        interval_list = scattered_intervals,
                        ref_fasta = ref_fasta,
                        ref_fasta_index = ref_fasta_index,
                        ref_dict = ref_dict,
                        filtered_vcf = variants_to_realign,
                        filtered_vcf_idx = variants_to_realign_idx,
                        compress_output = compress_output,
                        runtime_params = standard_runtime,
                        memoryMB = mem_select_variants,
                        runtime_minutes = time_startup + time_select_variants
                }

                call FilterAlignmentArtifacts {
                    input:
                        ref_fasta = ref_fasta,
                        ref_fasta_index = ref_fasta_index,
                        ref_dict = ref_dict,
                        tumor_bams = tumor_bams,
                        tumor_bais = tumor_bais,
                        input_vcf = SelectPreRealignmentVariants.selected_vcf,
                        input_vcf_idx = SelectPreRealignmentVariants.selected_vcf_idx,
                        bwa_mem_index_image = bwa_mem_index_image,
                        compress_output = compress_output,
                        max_reasonable_fragment_length = filter_alignment_artifacts_max_reasonable_fragment_length,
                        realignment_extra_args = realignment_extra_args,
                        runtime_params = standard_runtime,
                        memoryMB = mem_filter_alignment_artifacts + num_bams * mem_additional_per_sample,
                        runtime_minutes = time_startup + ceil(time_filter_alignment_artifacts_total / scatter_count),
                        cpu = filter_alignment_artifacts_cpu
                }
            }

            call MergeVCFs as MergeRealignmentFilteredVCFs {
                input:
                    input_vcfs = FilterAlignmentArtifacts.filtered_vcf,
                    input_vcf_indices = FilterAlignmentArtifacts.filtered_vcf_idx,
                    output_name = individual_id + ".filtered.selected.realignmentfiltered",
                    compress_output = compress_output,
                    runtime_params = standard_runtime,
                    memoryMB = mem_merge_vcfs,
                    runtime_minutes = time_startup + time_merge_vcfs
            }

            call SelectVariants as SelectPostRealignmentVariants {
                input:
                    filtered_vcf = MergeRealignmentFilteredVCFs.merged_vcf,
                    filtered_vcf_idx = MergeRealignmentFilteredVCFs.merged_vcf_idx,
                    exclude_filtered = true,
                    compress_output = compress_output,
                    select_variants_extra_args = select_variants_extra_args,
                    runtime_params = standard_runtime,
                    memoryMB = mem_select_variants,
                    runtime_minutes = time_startup + time_select_variants
            }

            if (run_realignment_filter_only_on_high_confidence_variants) {
                call MergeVCFs as MergeLowConfidenceAndRealignmentFilteredVCFs {
                    input:
                        input_vcfs = select_all([
                            SelectLowConfidenceVariants.selected_vcf,
                            SelectPostRealignmentVariants.selected_vcf
                        ]),
                        input_vcf_indices = select_all([
                            SelectLowConfidenceVariants.selected_vcf_idx,
                            SelectPostRealignmentVariants.selected_vcf_idx
                        ]),
                        output_name = individual_id + ".filtered.selected.realignmentfiltered.selected.merged",
                        compress_output = compress_output,
                        runtime_params = standard_runtime,
                        memoryMB = mem_merge_vcfs,
                        runtime_minutes = time_startup + time_merge_vcfs
                }
            }

            File realignment_filtered_variants = select_first([
                MergeLowConfidenceAndRealignmentFilteredVCFs.merged_vcf,
                SelectPostRealignmentVariants.selected_vcf
            ])
            File realignment_filtered_variants_idx = select_first([
                MergeLowConfidenceAndRealignmentFilteredVCFs.merged_vcf_idx,
                SelectPostRealignmentVariants.selected_vcf_idx
            ])
        }
    }

    File selected_vcf = select_first([
        realignment_filtered_variants,
        SelectPassingVariants.selected_vcf,
        MergeVariantCallVCFs.merged_vcf
    ])
    File selected_vcf_idx = select_first([
        realignment_filtered_variants_idx,
        SelectPassingVariants.selected_vcf_idx,
        MergeVariantCallVCFs.merged_vcf_idx
    ])

    String vcf_name = basename(basename(selected_vcf, ".gz"), ".vcf")

    if (make_bamout) {
        call MergeBamOuts {
            input:
                ref_fasta = ref_fasta,
                ref_fasta_index = ref_fasta_index,
                ref_dict = ref_dict,
                m2_bam_outs = select_all(VariantCall.bamout),
                output_vcf_name = vcf_name,
                runtime_params = standard_runtime,
                memoryMB = mem_merge_bams,
                runtime_minutes = time_startup + time_merge_bams
        }
    }

    if (run_cnn_scoring_model || run_funcotator) {
        # Funcotator is not designed to differentiate between samples. The tool uses the
        # position and allele information to get annotations but does not annotate per
        # sample. We use a workaround by splitting the multi-sample VCF into individual
        # samples and run Funcotator on each VCF.
        # In order to get proper annotations of somatic variants for the normal samples,
        # they should be called in tumor-only mode.

        scatter (tumor_sample in zip(zip(non_optional_tumor_sample_names, GetTumorSampleName.sample_name), zip(tumor_bams, tumor_bais))) {
            String assigned_tumor_sample_name =tumor_sample.left.left
            String tumor_bam_sample_name = tumor_sample.left.right
            File tumor_bam = tumor_sample.right.left
            File tumor_bai = tumor_sample.right.right

            # We select the first normal sample to be the matched normal.
            # todo: select normal with greatest sequencing depth
            # The default CNNScoreVariant model should not be used on VCFs with
            # annotations from a joint call-set. If the user chooses to run the CNN model,
            # we remove the matched normal sample from the VCF annotations. This only
            # results in the normal sample allelic coverage not being available for each
            # annotated variant.
            String? normal_sample_name = (
                if normal_is_present && !run_cnn_scoring_model
                then select_first(select_first([GetNormalSampleName.sample_name]))
                else None
            )

            call SelectVariants as SelectSampleVariants {
                input:
                    filtered_vcf = selected_vcf,
                    filtered_vcf_idx = selected_vcf_idx,
                    exclude_filtered = false,  # is already filtered
                    tumor_sample_name = tumor_bam_sample_name,
                    normal_sample_name = normal_sample_name,
                    compress_output = compress_output,
                    select_variants_extra_args = select_variants_extra_args,
                    runtime_params = standard_runtime,
                    memoryMB = mem_select_variants,
                    runtime_minutes = time_startup + time_select_variants
            }

            # CNNScoreVariants is very unreliable in its execution. It's probably
            # just grief you by constantly failing the workflow for no apparent
            # reason. This is still an issue for v4.3.0.0. You have been warned!
            if (run_cnn_scoring_model) {
                call CNNScoreVariants {
                    input:
                        ref_fasta = ref_fasta,
                        ref_fasta_index = ref_fasta_index,
                        ref_dict = ref_dict,
                        input_vcf = SelectSampleVariants.selected_vcf,
                        input_vcf_idx = SelectSampleVariants.selected_vcf_idx,
                        tumor_bam = select_first([MergeBamOuts.merged_bam_out, tumor_bam]),
                        tumor_bai = select_first([MergeBamOuts.merged_bam_out_index, tumor_bai]),
                        compress_output = compress_output,
                        runtime_params = standard_runtime,
                        memoryMB = mem_cnn_scoring,
                        runtime_minutes = time_startup + time_cnn_scoring,
                        cpu = cnn_scoring_cpu
                }
            }

            if (run_funcotator) {
                call Funcotate {
                    input:
                        ref_fasta = ref_fasta,
                        ref_fasta_index = ref_fasta_index,
                        ref_dict = ref_dict,
                        interval_list = evaluation_interval_list,
                        individual_id = individual_id,
                        input_vcf = select_first([CNNScoreVariants.scored_vcf, SelectSampleVariants.selected_vcf]),
                        input_vcf_idx = select_first([CNNScoreVariants.scored_vcf_idx, SelectSampleVariants.selected_vcf_idx]),
                        output_base_name = assigned_tumor_sample_name + ".annotated",
                        tumor_sample_name = assigned_tumor_sample_name,
                        normal_sample_name = normal_sample_name,
                        transcript_list = funcotator_transcript_list,
                        data_sources_tar_gz = funcotator_data_sources_tar_gz,
                        data_sources_paths = funcotator_data_sources_paths,
                        use_gnomad = funcotator_use_gnomad,
                        compress_output = compress_output,
                        reference_version = funcotator_reference_version,
                        output_format = funcotator_output_format,
                        variant_type = funcotator_variant_type,
                        transcript_selection_mode = funcotator_transcript_selection_mode,
                        annotation_defaults = funcotator_annotation_defaults,
                        annotation_overrides = funcotator_annotation_overrides,
                        exclude_fields = funcotator_exclude_fields,
                        funcotate_extra_args = funcotate_extra_args,
                        runtime_params = standard_runtime,
                        memoryMB = mem_funcotate,
                        runtime_minutes = time_startup + time_funcotate
                }
            }
        }
    }

    output {
        Array[File]? covered_intervals = EvaluationIntervals.covered_intervals
        File evaluation_intervals = evaluation_interval_list
        File unfiltered_vcf = MergeVariantCallVCFs.merged_vcf
        File unfiltered_vcf_idx = MergeVariantCallVCFs.merged_vcf_idx
        File merged_vcf = selected_vcf
        File merged_vcf_idx = selected_vcf_idx
        File mutect_stats = MergeMutectStats.merged_stats
        File? bamout = MergeBamOuts.merged_bam_out
        File? bamout_index = MergeBamOuts.merged_bam_out_index
        File? filtering_stats = FilterMutectCalls.filtering_stats
        File? read_orientation_model_params = LearnReadOrientationModel.orientation_bias
        Array[File]? contamination_table = CalculateContamination.contamination_table
        Array[File]? tumor_segmentation = CalculateContamination.tumor_segmentation
        Array[File?]? scored_file = CNNScoreVariants.scored_vcf
        Array[File?]? scored_file_idx = CNNScoreVariants.scored_vcf_idx
        Array[File?]? funcotated_file = Funcotate.funcotated_output_file
        Array[File?]? funcotated_file_index = Funcotate.funcotated_output_file_index
    }
}

task PreprocessIntervals {
    input {
        File? interval_list
        Array[File]? interval_lists
        File ref_fasta
        File ref_fasta_index
        File ref_dict

        Int bin_length = 0
        Int padding = 0
        String? preprocess_intervals_extra_args

        Runtime runtime_params
        Int? memoryMB
        Int? runtime_minutes
    }

    parameter_meta {
        interval_list: {localization_optional: true}
        interval_lists: {localization_optional: true}
        ref_fasta: {localization_optional: true}
        ref_fasta_index: {localization_optional: true}
        ref_dict: {localization_optional: true}
    }

    String preprocessed_intervals = "preprocessed.interval_list"

    command <<<
        set -e
        export GATK_LOCAL_JAR=~{default="/root/gatk.jar" runtime_params.gatk_override}
        gatk --java-options "-Xmx~{select_first([memoryMB, runtime_params.command_mem])}m" \
            PreprocessIntervals \
            -R '~{ref_fasta}' \
            ~{"-L '" + interval_list + "'"} \
            ~{true="-I '" false="" defined(interval_lists)}~{default="" sep="' -I '" interval_lists}~{true="'" false="" defined(interval_lists)} \
            --bin-length ~{bin_length} \
            --padding ~{padding} \
            --interval-merging-rule OVERLAPPING_ONLY \
            -O '~{preprocessed_intervals}' \
            ~{preprocess_intervals_extra_args}
    >>>

    output {
        File preprocessed_interval_list = preprocessed_intervals
    }

    runtime {
        docker: runtime_params.gatk_docker
        bootDiskSizeGb: runtime_params.boot_disk_size
        memory: select_first([memoryMB, runtime_params.machine_mem]) + " MB"
        runtime_minutes: select_first([runtime_minutes, runtime_params.runtime_minutes])
        disks: "local-disk " + runtime_params.disk + " HDD"
        preemptible: runtime_params.preemptible
        maxRetries: runtime_params.max_retries
        cpu: runtime_params.cpu
    }
}

task SplitIntervals {
    input {
        File? interval_list
        File ref_fasta
        File ref_fasta_index
        File ref_dict

        Int scatter_count
        String? split_intervals_extra_args

        Runtime runtime_params
        Int? memoryMB
        Int? runtime_minutes
    }

    String extra_args = (
        select_first([split_intervals_extra_args, ""])
        # to avoid splitting intervals:
        # + " --subdivision-mode BALANCING_WITHOUT_INTERVAL_SUBDIVISION_WITH_OVERFLOW"
        # Applied after inital scatter, so leads to more scattered intervals.
        # + " --dont-mix-contigs"
    )

    parameter_meta {
        interval_list: {localization_optional: true}
        ref_fasta: {localization_optional: true}
        ref_fasta_index: {localization_optional: true}
        ref_dict: {localization_optional: true}
    }

    command <<<
        set -e
        export GATK_LOCAL_JAR=~{default="/root/gatk.jar" runtime_params.gatk_override}
        mkdir interval-files
        gatk --java-options "-Xmx~{select_first([memoryMB, runtime_params.command_mem])}m" \
            SplitIntervals \
            -R '~{ref_fasta}' \
            ~{"-L '" + interval_list + "'"} \
            -scatter ~{scatter_count} \
            -O interval-files \
            ~{extra_args}
    >>>

    output {
        Array[File] interval_files = glob("interval-files/*.interval_list")
    }

    runtime {
        docker: runtime_params.gatk_docker
        bootDiskSizeGb: runtime_params.boot_disk_size
        memory: select_first([memoryMB, runtime_params.machine_mem]) + " MB"
        runtime_minutes: select_first([runtime_minutes, runtime_params.runtime_minutes])
        disks: "local-disk " + runtime_params.disk + " HDD"
        preemptible: runtime_params.preemptible
        maxRetries: runtime_params.max_retries
        cpu: runtime_params.cpu
    }
}

task GetSampleName {
    input {
        File bam

        Runtime runtime_params
        Int? memoryMB
        Int? runtime_minutes
    }

    parameter_meta {
        bam: {localization_optional: true}
    }

    command <<<
        set -e
        export GATK_LOCAL_JAR=~{default="/root/gatk.jar" runtime_params.gatk_override}
        gatk --java-options "-Xmx~{select_first([memoryMB, runtime_params.command_mem])}m" \
            GetSampleName \
            -I '~{bam}' \
            -O bam_name.txt
    >>>

    output {
        String sample_name = read_string("bam_name.txt")
    }

    runtime {
        docker: runtime_params.gatk_docker
        bootDiskSizeGb: runtime_params.boot_disk_size
        memory: select_first([memoryMB, runtime_params.machine_mem]) + " MB"
        runtime_minutes: select_first([runtime_minutes, runtime_params.runtime_minutes])
        disks: "local-disk " + runtime_params.disk + " HDD"
        preemptible: runtime_params.preemptible
        maxRetries: runtime_params.max_retries
        cpu: runtime_params.cpu
    }
}

task VariantCall {
    input {
        File? interval_list
        File ref_fasta
        File ref_fasta_index
        File ref_dict

        String individual_id
        Array[File] tumor_bams
        Array[File] tumor_bais
        Array[File]? normal_bams
        Array[File]? normal_bais
        Array[String]? normal_sample_names

        File? panel_of_normals
        File? panel_of_normals_idx
        File? germline_resource
        File? germline_resource_tbi

        Boolean genotype_germline_sites = false
        Boolean genotype_pon_sites = false
        Boolean native_pair_hmm_use_double_precision = true
        Boolean use_linked_de_bruijn_graph = true
        Boolean recover_all_dangling_branches = true

        # The linked de-Bruijn graph implementation has trouble calling variants
        # in complex regions. Reducing the downsampling by increasing the following
        # parameters might solve the issue. It increases compute cost though.
        Int downsampling_stride = 1
        Int max_reads_per_alignment_start = 50

        String? m2_extra_args

        Boolean run_ob_filter = false
        Boolean compress_output = false
        Boolean make_bamout = false

        File? gatk_override
        String gatk_docker
        Int preemptible = 2
        Int max_retries = 2
        Int cpu = 2  # to have 4 native_hmm_pair_threads
        Int memoryMB = 8192
        Int runtime_minutes = 60
        Int disk_spaceGB = 100
        Int boot_disk_size_GB = 12  # must be > 10
    }

    parameter_meta {
        interval_list: {localization_optional: true}
        ref_fasta: {localization_optional: true}
        ref_fasta_index: {localization_optional: true}
        ref_dict: {localization_optional: true}
        tumor_bams: {localization_optional: true}
        tumor_bais: {localization_optional: true}
        normal_bams: {localization_optional: true}
        normal_bais: {localization_optional: true}
        panel_of_normals: {localization_optional: true}
        panel_of_normals_idx: {localization_optional: true}
        germline_resource: {localization_optional: true}
        germline_resource_tbi: {localization_optional: true}
    }

    Boolean normal_is_present = defined(normal_bams) && (length(select_first([normal_bams])) > 0)

    String output_vcf = individual_id + if compress_output then ".vcf.gz" else ".vcf"
    String output_vcf_idx = output_vcf + if compress_output then ".tbi" else ".idx"
    String output_stats = output_vcf + ".stats"

    Int germline_resource_size = if defined(germline_resource) then ceil(size(germline_resource, "GB")) else 0
    Int disk_space = disk_spaceGB + germline_resource_size

    String output_bam = individual_id + "_bamout.bam"
    String output_bai = individual_id + "_bamout.bai"
    String make_bamout_arg = if make_bamout then "--bam-output " + output_bam else ""
    String output_artifact_priors = individual_id + "_f1r2_counts.tar.gz"
    String run_ob_filter_arg = if run_ob_filter then "--f1r2-tar-gz " + output_artifact_priors else ""

    command <<<
        set -e
        export GATK_LOCAL_JAR=~{default="/root/gatk.jar" gatk_override}
        gatk --java-options "-Xmx~{memoryMB}m" \
            Mutect2 \
            --reference '~{ref_fasta}' \
            ~{sep="' " prefix("-I '", tumor_bams)}' \
            ~{true="-I '" false="" normal_is_present}~{default="" sep="' -I '" normal_bams}~{true="'" false="" normal_is_present} \
            ~{true="-normal '" false="" normal_is_present}~{default="" sep="' -normal '" normal_sample_names}~{true="'" false="" normal_is_present} \
            --output '~{output_vcf}' \
            ~{"--intervals '" + interval_list + "'"} \
            ~{"-pon '" + panel_of_normals + "'"} \
            ~{make_bamout_arg} \
            ~{run_ob_filter_arg} \
            ~{"--germline-resource '" + germline_resource + "'"} \
            ~{true="--genotype-germline-sites true" false="" genotype_germline_sites} \
            ~{true="--genotype-pon-sites true" false="" genotype_pon_sites} \
            ~{true="--linked-de-bruijn-graph true" false="" use_linked_de_bruijn_graph} \
            ~{true="--recover-all-dangling-branches true" false="" recover_all_dangling_branches} \
            --pileup-detection true \
            --smith-waterman FASTEST_AVAILABLE \
            --pair-hmm-implementation FASTEST_AVAILABLE \
            ~{true="--native-pair-hmm-use-double-precision true" false="" native_pair_hmm_use_double_precision} \
            ~{"--downsampling-stride " + downsampling_stride} \
            ~{"--max-reads-per-alignment-start " + max_reads_per_alignment_start} \
            --seconds-between-progress-updates 300 \
            ~{m2_extra_args}
    >>>

    output {
        File vcf = output_vcf
        File vcf_idx = output_vcf_idx
        File vcf_stats = output_stats
        File? bamout = output_bam
        File? baiout = output_bai
        File? m2_artifact_priors = output_artifact_priors
    }

    runtime {
        docker: gatk_docker
        bootDiskSizeGb: boot_disk_size_GB
        memory: memoryMB + " MB"
        runtime_minutes: runtime_minutes
        disks: "local-disk " + disk_space + " HDD"
        preemptible: preemptible
        maxRetries: max_retries
        cpu: cpu
    }
}

task MergeMutectStats {
    input {
        Array[File]+ stats
        String individual_id

        Runtime runtime_params
        Int? memoryMB
        Int? runtime_minutes
    }

    # Optional localization leads to cromwell error.
    # parameter_meta {
    #     stats: {localization_optional: true}
    # }

    String output_name = individual_id + "_merged.stats"

    command <<<
        set -e
        export GATK_LOCAL_JAR=~{default="/root/gatk.jar" runtime_params.gatk_override}
        gatk --java-options "-Xmx~{select_first([memoryMB, runtime_params.command_mem])}m" \
            MergeMutectStats \
            ~{sep="' " prefix("-stats '", stats)}' \
            --output '~{output_name}'
    >>>

    output {
        File merged_stats = output_name
    }

    runtime {
        docker: runtime_params.gatk_docker
        bootDiskSizeGb: runtime_params.boot_disk_size
        memory: select_first([memoryMB, runtime_params.machine_mem]) + " MB"
        runtime_minutes: select_first([runtime_minutes, runtime_params.runtime_minutes])
        disks: "local-disk " + runtime_params.disk + " HDD"
        preemptible: runtime_params.preemptible
        maxRetries: runtime_params.max_retries
        cpu: runtime_params.cpu
    }
}

task LearnReadOrientationModel {
    # This tool uses the forward and reverse read counts collected in the variant call
    # workflow. The basic idea of this tool is that true variants will be supported by
    # reads on both strands of DNA; whereas, artifacts will be supported by reads heavily
    # biased to one of the two strands. CollectF1R2 counts the number of reads on each
    # strand of DNA at a putatively variant site and stores this information along with
    # the nucleotide context. LearnReadOrientationModel uses the information from CollectF1R2
    # to estimate a prior probability that a site with a given context suffers from an
    # artifact. The two main artifacts of concern are OXOG (a result of sequencing technology)
    # and FFPE.

    input {
        String individual_id
        Array[File] f1r2_counts

        Runtime runtime_params
        Int? memoryMB
        Int? runtime_minutes
    }

    # Optional localization leads to cromwell error.
    # parameter_meta {
    #     f1r2_counts: {localization_optional: true}
    # }

    Int f1r2_counts_size = ceil(size(f1r2_counts, "GB"))
    Int disk_size = runtime_params.disk + f1r2_counts_size
    String output_name = individual_id + "_artifact_priors.tar.gz"
    Boolean f1r2_counts_empty = (length(f1r2_counts) == 0)

    command <<<
        set -e
        export GATK_LOCAL_JAR=~{default="/root/gatk.jar" runtime_params.gatk_override}

        if ~{f1r2_counts_empty} ; then
            echo "ERROR: f1r2_counts_tar_gz must be supplied and non empty."
            false
        fi

        gatk --java-options "-Xmx~{select_first([memoryMB, runtime_params.command_mem])}m" \
            LearnReadOrientationModel \
            ~{sep="' " prefix("-I '", f1r2_counts)}' \
            --output '~{output_name}'
    >>>

    output {
        File orientation_bias = output_name
    }

    runtime {
        docker: runtime_params.gatk_docker
        bootDiskSizeGb: runtime_params.boot_disk_size
        memory: select_first([memoryMB, runtime_params.machine_mem]) + " MB"
        runtime_minutes: select_first([runtime_minutes, runtime_params.runtime_minutes])
        disks: "local-disk " + disk_size + " HDD"
        preemptible: runtime_params.preemptible
        maxRetries: runtime_params.max_retries
        cpu: runtime_params.cpu
    }
}

task GetPileupSummaries {
    # If the variants for contamination and the intervals for this scatter don't
    # intersect, GetPileupSummaries throws an error. However, there is nothing wrong
    # with an empty intersection for our purposes; it simply doesn't contribute to the
    # merged pileup summaries that we create downstream. We implement this with an array
    # outputs. If the tool errors, no table is created and the glob yields an empty array.

	input {
        File interval_list
        File input_bam
        File input_bai
        File? variants_for_contamination
        File? variants_for_contamination_idx
        String? getpileupsummaries_extra_args

        Runtime runtime_params
        Int? memoryMB
        Int? runtime_minutes
        Int? disk_spaceGB
	}

    parameter_meta {
        interval_list: {localization_optional: true}
        input_bam: {localization_optional: true}
        input_bai: {localization_optional: true}
        variants_for_contamination: {localization_optional: true}
        variants_for_contamination_idx: {localization_optional: true}
    }

    String sample_id = basename(input_bam, ".bam")
    String output_name = sample_id + ".pileup"

    command <<<
        set +e
        export GATK_LOCAL_JAR=~{default="/root/gatk.jar" runtime_params.gatk_override}

        if ~{!defined(variants_for_contamination)} ; then
            echo "ERROR: variants_for_contamination must be supplied."
            false
        fi

        gatk --java-options "-Xmx~{select_first([memoryMB, runtime_params.command_mem])}m" \
            GetPileupSummaries \
            --input '~{input_bam}' \
            --intervals '~{interval_list}' \
            --intervals '~{variants_for_contamination}' \
            --interval-set-rule INTERSECTION \
            --variant '~{variants_for_contamination}' \
            --output '~{output_name}' \
            ~{getpileupsummaries_extra_args}

        # It only fails due to empty intersection between variants and intervals, which is ok.
        exit 0
    >>>

    # need to use glob in case GetPileupSummaries fails and does not produce output files
    output {
        Array[File] pileup_summaries = glob(output_name)
    }

    runtime {
        docker: runtime_params.gatk_docker
        bootDiskSizeGb: runtime_params.boot_disk_size
        memory: select_first([memoryMB, runtime_params.machine_mem]) + " MB"
        runtime_minutes: select_first([runtime_minutes, runtime_params.runtime_minutes])
        disks: "local-disk " + select_first([disk_spaceGB, runtime_params.disk]) + " HDD"
        preemptible: runtime_params.preemptible
        maxRetries: runtime_params.max_retries
        cpu: runtime_params.cpu
    }
}

task GatherPileupSummaries {
    input {
        Array[File] input_tables
        File ref_dict
        String output_name

        Runtime runtime_params
        Int? memoryMB
        Int? runtime_minutes
    }

    # Optional localization leads to cromwell error.
    # parameter_meta {
    #     input_tables: {localization_optional: true}
    #     ref_dict: {localization_optional: true}
    # }

    command <<<
        set -e
        export GATK_LOCAL_JAR=~{default="/root/gatk.jar" runtime_params.gatk_override}
        gatk --java-options "-Xmx~{select_first([memoryMB, runtime_params.command_mem])}m" \
            GatherPileupSummaries \
            --sequence-dictionary '~{ref_dict}' \
            ~{sep="' " prefix("-I '", input_tables)}' \
            -O '~{output_name}'
    >>>

    output {
        File merged_pileup_summaries = output_name
    }

    runtime {
        docker: runtime_params.gatk_docker
        bootDiskSizeGb: runtime_params.boot_disk_size
        memory: select_first([memoryMB, runtime_params.machine_mem]) + " MB"
        runtime_minutes: select_first([runtime_minutes, runtime_params.runtime_minutes])
        disks: "local-disk " + runtime_params.disk + " HDD"
        preemptible: runtime_params.preemptible
        maxRetries: runtime_params.max_retries
        cpu: runtime_params.cpu
    }
}

task CalculateContamination {
    # This tool is set, by default, to operate over the intersection of your germline
    # biallelic resource with your specified intervals. The tool works by pulling down
    # alt and ref allele counts at locuses defined in the biallelic_germline_resource
    # and comparing the number of observed alt counts compared to the number of expected
    # alt counts at sites with low population allele frequencies. Using this comparison
    # of observed vs expected, the tool estimates contamination. It gains further
    # accuracy if a matched normal is specified.

    input {
        File tumor_pileups
        File? normal_pileups

        Runtime runtime_params
        Int? memoryMB
        Int? runtime_minutes
    }

    # Optional localization leads to cromwell error.
    # parameter_meta {
    #     tumor_pileups: {localization_optional: true}
    #     normal_pileups: {localization_optional: true}
    # }

    String tumor_sample_id = basename(tumor_pileups, ".pileup")
    String normal_sample_id = if defined(normal_pileups) then "." + basename(select_first([normal_pileups]), ".pileup") else ""
    String output_contamination = tumor_sample_id + normal_sample_id + ".contamination"
    String output_segments = tumor_sample_id + normal_sample_id + ".segments"

    command <<<
        set -e
        export GATK_LOCAL_JAR=~{default="/root/gatk.jar" runtime_params.gatk_override}
        gatk --java-options "-Xmx~{select_first([memoryMB, runtime_params.command_mem])}m" \
            CalculateContamination \
            --input '~{tumor_pileups}' \
            ~{"--matched-normal '" + normal_pileups + "'"} \
            --output '~{output_contamination}' \
            --tumor-segmentation '~{output_segments}'
    >>>

    output {
        File contamination_table = output_contamination
        File tumor_segmentation = output_segments
    }

    runtime {
        docker: runtime_params.gatk_docker
        bootDiskSizeGb: runtime_params.boot_disk_size
        memory: select_first([memoryMB, runtime_params.machine_mem]) + " MB"
        runtime_minutes: select_first([runtime_minutes, runtime_params.runtime_minutes])
        disks: "local-disk " + runtime_params.disk + " HDD"
        preemptible: runtime_params.preemptible
        maxRetries: runtime_params.max_retries
        cpu: runtime_params.cpu
    }
}

task FilterMutectCalls {
    # Passing variants are labelled with PASS annotation.

    input {
        File ref_fasta
        File ref_fasta_index
        File ref_dict

        File input_vcf
        File input_vcf_idx
        File? orientation_bias
        Array[File]? contamination_tables
        Array[File]? tumor_segmentation
        File? mutect_stats

        Int max_median_fragment_length_difference = 10000  # default: 10000
        Int min_alt_median_base_quality = 20  # default: 20
        Int min_alt_median_mapping_quality = 20  # default: -1

        Boolean compress_output = false
        String? m2_filter_extra_args

        Runtime runtime_params
        Int? memoryMB
        Int? runtime_minutes
    }

    # Optional localization leads to cromwell error.
    parameter_meta{
        ref_fasta: {localization_optional: true}
        ref_fasta_index: {localization_optional: true}
        ref_dict: {localization_optional: true}
        input_vcf: {localization_optional: true}
        input_vcf_idx: {localization_optional: true}
        # orientation_bias: {localization_optional: true}
        # contamination_tables: {localization_optional: true}
        # tumor_segmentation: {localization_optional: true}
        # mutect_stats: {localization_optional: true}
    }

    Int disk_spaceGB = (
        ceil(size(input_vcf, "GB"))
        + if defined(orientation_bias) then ceil(size(orientation_bias, "GB")) else 0
        + if defined(contamination_tables) then ceil(size(contamination_tables, "GB")) else 0
        + if defined(tumor_segmentation) then ceil(size(tumor_segmentation, "GB")) else 0
        + if defined(mutect_stats) then ceil(size(mutect_stats, "GB")) else 0
        + runtime_params.disk
    )

    String output_base_name = basename(basename(input_vcf, ".gz"), ".vcf") + ".filtered"
    String output_vcf = output_base_name + if compress_output then ".vcf.gz" else ".vcf"
    String output_vcf_idx = output_vcf + if compress_output then ".tbi" else ".idx"

    command <<<
        set -e
        export GATK_LOCAL_JAR=~{default="/root/gatk.jar" runtime_params.gatk_override}
        gatk --java-options "-Xmx~{select_first([memoryMB, runtime_params.command_mem])}m" \
            FilterMutectCalls \
            --reference '~{ref_fasta}' \
            --variant '~{input_vcf}' \
            --output '~{output_vcf}' \
            ~{"--orientation-bias-artifact-priors '" + orientation_bias + "'"} \
            ~{true="--contamination-table '" false="" defined(contamination_tables)}~{default="" sep="' --contamination-table '" contamination_tables}~{true="'" false="" defined(contamination_tables)} \
            ~{true="--tumor-segmentation '" false="" defined(tumor_segmentation)}~{default="" sep="' --tumor-segmentation '" tumor_segmentation}~{true="'" false="" defined(tumor_segmentation)} \
            ~{"--stats '" + mutect_stats + "'"} \
            ~{"--max-median-fragment-length-difference " + max_median_fragment_length_difference} \
            ~{"--min-median-base-quality " + min_alt_median_base_quality} \
            ~{"--min-median-mapping-quality " + min_alt_median_mapping_quality} \
            --filtering-stats '~{output_base_name}.stats' \
            --seconds-between-progress-updates 300 \
            ~{m2_filter_extra_args}
    >>>

    output {
        File filtered_vcf = output_vcf
        File filtered_vcf_idx = output_vcf_idx
        File filtering_stats = output_base_name + ".stats"
    }

    runtime {
        docker: runtime_params.gatk_docker
        bootDiskSizeGb: runtime_params.boot_disk_size
        memory: select_first([memoryMB, runtime_params.machine_mem]) + " MB"
        runtime_minutes: select_first([runtime_minutes, runtime_params.runtime_minutes])
        disks: "local-disk " + disk_spaceGB + " HDD"
        preemptible: runtime_params.preemptible
        maxRetries: runtime_params.max_retries
        cpu: runtime_params.cpu
    }
}

task FilterAlignmentArtifacts {
    # Lifts the reads supporting the variant alleles from the tumor bams and aligns
    # these reads to Hg38. If the reads supporting the variant allele have ambiguous
    # alignments, the tool will mark these variants as filtered in the VCF.

    input {
        File ref_fasta
        File ref_fasta_index
        File ref_dict
        File input_vcf
        File input_vcf_idx
        Array[File]+ tumor_bams
        Array[File]+ tumor_bais

        File? bwa_mem_index_image

        Int max_reasonable_fragment_length = 10000 # default: 100000
        Boolean compress_output = false
        String? realignment_extra_args

        Runtime runtime_params
        String? gatk_docker
        Int? memoryMB
        Int? runtime_minutes
        Int? cpu = 2  # wants 4
    }

    parameter_meta {
        input_vcf: {localization_optional: true}
        input_vcf_idx: {localization_optional: true}
        tumor_bams: {localization_optional: true}
        tumor_bais: {localization_optional: true}
        ref_fasta: {localization_optional: true}
        ref_fasta_index: {localization_optional: true}
        ref_dict: {localization_optional: true}
        # bwa_mem_index_image: {localization_optional: true}  # needs to be localized
    }

    Int disk_spaceGB = ceil(size(bwa_mem_index_image, "GB")) + runtime_params.disk

    String output_base_name = basename(basename(input_vcf, ".gz"), ".vcf") + ".realignmentfiltered"
    String output_vcf = output_base_name + if compress_output then ".vcf.gz" else ".vcf"
    String output_vcf_idx = output_vcf + if compress_output then ".tbi" else ".idx"

    command <<<
        set -e
        export GATK_LOCAL_JAR=~{default="/root/gatk.jar" runtime_params.gatk_override}

        if ~{!defined(bwa_mem_index_image)} ; then
            echo "ERROR: bwa_mem_index_image must be supplied."
            false
        fi

        # Not skipping filtered variants is important for keeping germline variants if
        # requested.
        gatk --java-options "-Xmx~{select_first([memoryMB, runtime_params.command_mem])}m" \
            FilterAlignmentArtifacts \
            ~{sep="' " prefix("-I '", tumor_bams)}' \
            --variant '~{input_vcf}' \
            --reference '~{ref_fasta}' \
            --bwa-mem-index-image '~{bwa_mem_index_image}' \
            --output '~{output_vcf}' \
            --max-reasonable-fragment-length ~{max_reasonable_fragment_length} \
            --dont-skip-filtered-variants true \
            ~{realignment_extra_args}
    >>>

    output {
        File filtered_vcf = output_vcf
        File filtered_vcf_idx = output_vcf_idx
    }

    runtime {
        docker: select_first([gatk_docker, runtime_params.gatk_docker])
        bootDiskSizeGb: runtime_params.boot_disk_size
        memory: select_first([memoryMB, runtime_params.machine_mem]) + " MB"
        runtime_minutes: select_first([runtime_minutes, runtime_params.runtime_minutes])
        disks: "local-disk " + disk_spaceGB + " HDD"
        preemptible: runtime_params.preemptible
        maxRetries: runtime_params.max_retries
        cpu: cpu
    }
}

task SelectVariants {
    # Selects only variants with the PASS annotation given in Filter Mutect Calls.
    # todo: add functionality to keep germline variants

    input {
        File? interval_list
        File? ref_fasta
        File? ref_fasta_index
        File? ref_dict
        File filtered_vcf
        File filtered_vcf_idx
        Boolean exclude_filtered = false
        Boolean compress_output = false
        String? tumor_sample_name
        String? normal_sample_name
        String? select_variants_extra_args

        Runtime runtime_params
        Int? memoryMB
        Int? runtime_minutes
    }

    parameter_meta {
        ref_fasta: {localization_optional: true}
        ref_fasta_index: {localization_optional: true}
        ref_dict: {localization_optional: true}
        filtered_vcf: {localization_optional: true}
        filtered_vcf_idx: {localization_optional: true}
    }

    String uncompressed_filtered_vcf = basename(filtered_vcf, ".gz")
    String base_name = if defined(tumor_sample_name) then sub(select_first([tumor_sample_name, ""]), " ", "+") else basename(uncompressed_filtered_vcf, ".vcf")
    String output_base_name = base_name + ".selected"
    String uncompressed_output_vcf = output_base_name + ".vcf"
    String output_vcf = uncompressed_output_vcf + if compress_output then ".gz" else ""
    String output_vcf_idx = output_vcf + if compress_output then ".tbi" else ".idx"

    String dollar = "$"
    # Remove variants from multi-sample calling that are not present in the selected
    # tumor sample. We demand that the total read depth is greater than the allelic
    # depth of the reference allele. This ensures that there is at least one alternate
    # allele present.
    String select_true_variants_arg = (
        if defined(tumor_sample_name)
        then "-select 'vc.getGenotype(\"" + tumor_sample_name + "\").getAD().0 < vc.getGenotype(\"" + tumor_sample_name + "\").getDP()'"
        else ""
    )

    command <<<
        set -e
        export GATK_LOCAL_JAR=~{default="/root/gatk.jar" runtime_params.gatk_override}
        gatk --java-options "-Xmx~{select_first([memoryMB, runtime_params.command_mem])}m" \
            SelectVariants \
            ~{"-R '" + ref_fasta + "'"} \
            ~{"-L '" + interval_list + "'"} \
            -V '~{filtered_vcf}' \
            --output '~{uncompressed_output_vcf}' \
            --exclude-filtered ~{exclude_filtered} \
            ~{"--sample-name '" + tumor_sample_name + "'"} \
            ~{"--sample-name '" + normal_sample_name + "'"} \
            ~{select_true_variants_arg} \
            ~{select_variants_extra_args}

        # =======================================
        # Hack to correct a SelectVariants output bug. When selecting for samples, this
        # task only retains the first sample annotation in the header. Those annotations
        # are important for Funcotator to fill the t_alt_count and t_ref_count coverage
        # columns.
        # Note: This can possibly be achieved by adding an appropriate --COMMENT flag.

        if ~{defined(tumor_sample_name)} ; then
            echo ">> Fixing tumor sample name in vcf header ... "
            input_header=~{dollar}(grep "##tumor_sample=" '~{uncompressed_output_vcf}')
            corrected_header="##tumor_sample=~{tumor_sample_name}"
            sed -i "s/~{dollar}input_header/~{dollar}corrected_header/g" '~{uncompressed_output_vcf}'
        fi
        if ~{defined(normal_sample_name)} ; then
            echo ">> Fixing normal sample name in vcf header ... "
            input_header=~{dollar}(grep "##normal_sample=" '~{uncompressed_output_vcf}')
            corrected_header="##normal_sample=~{normal_sample_name}"
            sed -i "s/~{dollar}input_header/~{dollar}corrected_header/g" '~{uncompressed_output_vcf}'
        fi
        if ~{compress_output} ; then
            echo ">> Compressing selected vcf."
            bgzip -c '~{uncompressed_output_vcf}' > '~{output_vcf}'
            gatk --java-options "-Xmx~{select_first([memoryMB, runtime_params.command_mem])}m" \
                IndexFeatureFile \
                --input '~{output_vcf}' \
                --output '~{output_vcf_idx}'
            rm '~{uncompressed_output_vcf}'
            rm '~{uncompressed_output_vcf}.idx'
        fi
    >>>

    output {
        File selected_vcf = output_vcf
        File selected_vcf_idx = output_vcf_idx
    }

    runtime {
        docker: runtime_params.gatk_docker
        bootDiskSizeGb: runtime_params.boot_disk_size
        memory: select_first([memoryMB, runtime_params.machine_mem]) + " MB"
        runtime_minutes: select_first([runtime_minutes, runtime_params.runtime_minutes])
        disks: "local-disk " + runtime_params.disk + " HDD"
        preemptible: runtime_params.preemptible
        maxRetries: runtime_params.max_retries
        cpu: runtime_params.cpu
    }
}

task MergeVCFs {
    # Consider replacing MergeVcfs with GatherVcfsCloud once the latter is out of beta.

	input {
        Array[File] input_vcfs
        Array[File] input_vcf_indices
        String output_name
        Boolean compress_output = false

        Runtime runtime_params
        Int? memoryMB
        Int? runtime_minutes
    }

    # Optional localization leads to cromwell error.
    # parameter_meta {
    #     input_vcfs: {localization_optional: true}
    #     input_vcf_indices: {localization_optional: true}
    # }

    String output_vcf = output_name + ".vcf" + if compress_output then ".gz" else ""
    String output_vcf_idx = output_vcf + if compress_output then ".tbi" else ".idx"

    command <<<
        set -e
        export GATK_LOCAL_JAR=~{default="/root/gatk.jar" runtime_params.gatk_override}
        gatk --java-options "-Xmx~{select_first([memoryMB, runtime_params.command_mem])}m" \
            MergeVcfs \
            ~{sep="' " prefix("-I '", input_vcfs)}' \
            -O '~{output_vcf}'
    >>>

    output {
    	File merged_vcf = output_vcf
        File merged_vcf_idx = output_vcf_idx
    }

    runtime {
        docker: runtime_params.gatk_docker
        bootDiskSizeGb: runtime_params.boot_disk_size
        memory: select_first([memoryMB, runtime_params.machine_mem]) + " MB"
        runtime_minutes: runtime_minutes
        disks: "local-disk " + runtime_params.disk + " HDD"
        preemptible: runtime_params.preemptible
        maxRetries: runtime_params.max_retries
        cpu: runtime_params.cpu
    }
}

task MergeBamOuts {
    input {
        File ref_fasta
        File ref_fasta_index
        File ref_dict
        Array[File]+ m2_bam_outs
        String output_vcf_name

        Runtime runtime_params
        Int? memoryMB
        Int? runtime_minutes
        Boolean use_ssd = false
    }

    parameter_meta {
        ref_fasta: {localization_optional: true}
        ref_fasta_index: {localization_optional: true}
        ref_dict: {localization_optional: true}
        # m2_bam_outs: {localization_optional: true}  # samtools requires localization
    }

    Int disk_spaceGB = 4 * ceil(size(m2_bam_outs, "GB")) + runtime_params.disk

    command <<<
        set -e
        export GATK_LOCAL_JAR=~{default="/root/gatk.jar" runtime_params.gatk_override}
        gatk --java-options "-Xmx~{select_first([memoryMB, runtime_params.command_mem])}m" \
            GatherBamFiles \
            ~{sep="' " prefix("-I '", m2_bam_outs)}' \
            -O unsorted.out.bam \
            -R '~{ref_fasta}'

        # We must sort because adjacent scatters may have overlapping (padded) assembly
        # regions, hence overlapping bamouts

        gatk --java-options "-Xmx~{select_first([memoryMB, runtime_params.command_mem])}m" \
            SortSam \
            -I unsorted.out.bam \
            -O '~{output_vcf_name}.out.bam' \
            --SORT_ORDER coordinate \
            -VALIDATION_STRINGENCY LENIENT

        gatk --java-options "-Xmx~{select_first([memoryMB, runtime_params.command_mem])}m" \
            BuildBamIndex \
            -I '~{output_vcf_name}.out.bam' \
            -VALIDATION_STRINGENCY LENIENT
    >>>

    output {
        File merged_bam_out = "~{output_vcf_name}.out.bam"
        File merged_bam_out_index = "~{output_vcf_name}.out.bai"
    }

    runtime {
        docker: runtime_params.gatk_docker
        bootDiskSizeGb: runtime_params.boot_disk_size
        memory: select_first([memoryMB, runtime_params.machine_mem]) + " MB"
        runtime_minutes: select_first([runtime_minutes, runtime_params.runtime_minutes])
        disks: "local-disk " + disk_spaceGB + if use_ssd then " SSD" else " HDD"
        preemptible: runtime_params.preemptible
        maxRetries: runtime_params.max_retries
        cpu: runtime_params.cpu
    }
}

task CNNScoreVariants {
    # Apply a Convolutional Neural Net to filter annotated variants.
    # The default models were trained on single-sample VCFs. The default model should
    # not be used on VCFs with annotations from joint call-sets.

    input {
        File ref_fasta
        File ref_fasta_index
        File ref_dict
        File input_vcf
        File input_vcf_idx
        File tumor_bam
        File tumor_bai

        Boolean compress_output = false
        String? cnn_score_variants_extra_args

        Runtime runtime_params
        String? gatk_docker
        Int? memoryMB
        Int? runtime_minutes
        Int? cpu = 1  # wants 4
    }

    parameter_meta {
        ref_fasta: {localization_optional: true}
        ref_fasta_index: {localization_optional: true}
        ref_dict: {localization_optional: true}
        input_vcf: {localization_optional: true}
        input_vcf_idx: {localization_optional: true}
        tumor_bam: {localization_optional: true}
        tumor_bai: {localization_optional: true}
    }

    Int disk_spaceGB = runtime_params.disk

    String output_base_name = basename(basename(input_vcf, ".gz"), ".vcf") + ".cnn_scored"
    String output_vcf = output_base_name + if compress_output then ".vcf.gz" else ".vcf"
    String output_vcf_idx = output_vcf + if compress_output then ".tbi" else ".idx"

    command <<<
        set -e
        export GATK_LOCAL_JAR=~{default="/root/gatk.jar" runtime_params.gatk_override}
        gatk --java-options "-Xmx~{select_first([memoryMB, runtime_params.command_mem])}m" \
            CNNScoreVariants \
            -I '~{tumor_bam}' \
            --variant '~{input_vcf}' \
            --reference '~{ref_fasta}' \
            --output '~{output_vcf}' \
            -tensor-type read_tensor \
            ~{cnn_score_variants_extra_args}
    >>>

  	output {
  		File scored_vcf = output_vcf
  		File scored_vcf_idx = output_vcf_idx
  	}

    runtime {
        docker: select_first([gatk_docker, runtime_params.gatk_docker])
        bootDiskSizeGb: runtime_params.boot_disk_size
        memory: select_first([memoryMB, runtime_params.machine_mem]) + " MB"
        runtime_minutes: select_first([runtime_minutes, runtime_params.runtime_minutes])
        disks: "local-disk " + disk_spaceGB + " HDD"
        preemptible: runtime_params.preemptible
        maxRetries: runtime_params.max_retries
        cpu: cpu
    }
}

task Funcotate {
    input {
        File ref_fasta
        File ref_fasta_index
        File ref_dict
        File? interval_list

        File input_vcf
        File input_vcf_idx
        String? individual_id
        String? tumor_sample_name
        String? normal_sample_name

        String reference_version = "hg19"
        String output_base_name
        String output_format = "MAF"
        String variant_type = "somatic"  # alternative: germline
        String transcript_selection_mode = "CANONICAL"  # GATK default: "CANONICAL"
        File? transcript_list
        File? data_sources_tar_gz  # most recent version is downloaded if not chosen
        Boolean use_gnomad = false
        Boolean compress_output = false
        Array[String]? data_sources_paths
        Array[String]? annotation_defaults
        Array[String]? annotation_overrides
        Array[String]? exclude_fields
        String? funcotate_extra_args

        Runtime runtime_params
        Int? memoryMB
        Int? runtime_minutes
        Int? disk_spaceGB
        Boolean use_ssd = false
    }

    parameter_meta {
        ref_fasta: {localization_optional: true}
        ref_fasta_index: {localization_optional: true}
        ref_dict: {localization_optional: true}
        interval_list: {localization_optional: true}
        input_vcf: {localization_optional: true}
        input_vcf_idx: {localization_optional: true}
        # transcript_selection_file: {localization_optional: true}
    }

    # ==============
    # Process input args:
    String output_maf = output_base_name + ".maf"
    String output_maf_index = output_maf + ".idx"
    String output_vcf = output_base_name + if compress_output then ".vcf.gz" else ".vcf"
    String output_vcf_idx = output_vcf +  if compress_output then ".tbi" else ".idx"
    String output_file = if output_format == "MAF" then output_maf else output_vcf
    String output_file_index = if output_format == "MAF" then output_maf_index else output_vcf_idx

    # Calculate disk size:
    Int funco_tar_sizeGB = if defined(data_sources_paths) then 0 else (if defined(data_sources_tar_gz) then 4 * ceil(size(data_sources_tar_gz, "GB")) else 100)
    Int diskGB = funco_tar_sizeGB + select_first([disk_spaceGB, runtime_params.disk])

    String dollar = "$"

    command <<<
        set -e
        export GATK_LOCAL_JAR=~{default="/root/gatk.jar" runtime_params.gatk_override}

        # =======================================
        # Hack to validate the WDL inputs:
        #
        # NOTE: This happens here so that we don't waste time copying down the data
        # sources if there's an error.

        if [[ "~{output_format}" != "MAF" ]] && [[ "~{output_format}" != "VCF" ]] ; then
            echo "ERROR: Output format must be MAF or VCF."
            false
        fi

        if ~{!defined(data_sources_paths)} ; then
            mkdir datasources_dir
            DATA_SOURCES_FOLDER="$PWD/datasources_dir"
            echo "Obtaining Funcotator data sources..."
            if [[ ! -z "~{data_sources_tar_gz}" ]]; then
                data_sources_tar_gz=~{data_sources_tar_gz}
            else
                data_sources_tar_gz="funcotator_datasources.tar.gz"
                gatk --java-options "-Xmx~{select_first([memoryMB, runtime_params.command_mem])}m" \
                    FuncotatorDataSourceDownloader \
                    --~{variant_type} \
                    --validate-integrity \
                    --output ~{dollar}data_sources_tar_gz
            fi
            echo "Unzipping Funcotator data sources..."
            tar zxvf ~{dollar}data_sources_tar_gz -C datasources_dir --strip-components 1

            if ~{use_gnomad} ; then
                echo "Enabling gnomAD..."
                for potential_gnomad_gz in gnomAD_exome.tar.gz gnomAD_genome.tar.gz ; do
                    if [[ -f ~{dollar}{DATA_SOURCES_FOLDER}/~{dollar}{potential_gnomad_gz} ]] ; then
                        cd ~{dollar}{DATA_SOURCES_FOLDER}
                        tar -zvxf ~{dollar}{potential_gnomad_gz}
                        cd -
                    else
                        echo "ERROR: Cannot find gnomAD folder: ~{dollar}{potential_gnomad_gz}" 1>&2
                        false
                    fi
                done
            fi
        fi

        gatk --java-options "-Xmx~{select_first([memoryMB, runtime_params.command_mem])}m" \
            Funcotator \
            --data-sources-path ~{true="" false="$DATA_SOURCES_FOLDER" defined(data_sources_paths)}~{default="" sep=" --data-sources-path " data_sources_paths} \
            --ref-version ~{reference_version} \
            --output-file-format ~{output_format} \
            -R '~{ref_fasta}' \
            -V '~{input_vcf}' \
            -O '~{output_file}' \
            ~{"-L '" + interval_list + "'"} \
            ~{"--transcript-selection-mode " + transcript_selection_mode} \
            ~{"--transcript-list '" + transcript_list + "'"} \
            --annotation-default 'individual_id:~{default="Unknown" individual_id}' \
            --annotation-default 'tumor_barcode:~{default="Unknown" tumor_sample_name}' \
            --annotation-default 'normal_barcode:~{default="Unknown" normal_sample_name}' \
            ~{true="--annotation-default " false="" defined(annotation_defaults)}~{default="" sep=" --annotation-default " annotation_defaults} \
            ~{true="--annotation-override " false="" defined(annotation_overrides)}~{default="" sep=" --annotation-override " annotation_overrides} \
            ~{true="--exclude-field " false="" defined(exclude_fields)}~{default="" sep=" --exclude-field " exclude_fields} \
            ~{funcotate_extra_args}

        rm -r datasources_dir
    >>>

    output {
        File funcotated_output_file = output_file
        File? funcotated_output_file_index = output_file_index
    }

    runtime {
        docker: runtime_params.gatk_docker
        bootDiskSizeGb: runtime_params.boot_disk_size
        memory: select_first([memoryMB, runtime_params.machine_mem]) + " MB"
        runtime_minutes: select_first([runtime_minutes, runtime_params.runtime_minutes])
        disks: "local-disk " + diskGB + if use_ssd then " SSD" else " HDD"
        preemptible: runtime_params.preemptible
        maxRetries: runtime_params.max_retries
        cpu: runtime_params.cpu
    }
}